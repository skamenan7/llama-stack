[build-system]
requires = ["setuptools>=61.0", "setuptools-scm>=8.0"]
build-backend = "setuptools.build_meta"

[tool.uv]
required-version = ">=0.7.0"

[project]
name = "llama-stack-api"
dynamic = ["version"]
authors = [{ name = "Meta Llama", email = "llama-oss@meta.com" }]
description = "API and Provider specifications for Llama Stack - lightweight package with protocol definitions and provider specs"
readme = "README.md"
requires-python = ">=3.12"
license = { "text" = "MIT" }
classifiers = [
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Operating System :: OS Independent",
    "Intended Audience :: Developers",
    "Intended Audience :: Information Technology",
    "Intended Audience :: Science/Research",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Scientific/Engineering :: Information Analysis",
]
dependencies = [
    "openai>=2.5.0",
    "fastapi>=0.115.0,<1.0",
    "pydantic>=2.11.9",
    "jsonschema",
    "opentelemetry-sdk>=1.30.0",
    "opentelemetry-exporter-otlp-proto-http>=1.30.0",
]

[project.urls]
Homepage = "https://github.com/llamastack/llama-stack"

[tool.setuptools]
# 1. Map the 'llama_stack_api' namespace to the current directory (.)
# This anchors the package and ensures it's not empty.
package-dir = {"llama_stack_api" = "."}

# 2. List ONLY the directories as sub-packages.
# WARNING: Update this list when adding/removing API directories (e.g., new FastAPI routers)
packages = [
    "llama_stack_api.admin",
    "llama_stack_api.agents",
    "llama_stack_api.batches",
    "llama_stack_api.benchmarks",
    "llama_stack_api.common",
    "llama_stack_api.conversations",
    "llama_stack_api.datasetio",
    "llama_stack_api.datasets",
    "llama_stack_api.eval",
    "llama_stack_api.file_processors",
    "llama_stack_api.files",
    "llama_stack_api.inspect_api",
    "llama_stack_api.inference",
    "llama_stack_api.internal",
    "llama_stack_api.models",
    "llama_stack_api.post_training",
    "llama_stack_api.providers",
    "llama_stack_api.shields",
    "llama_stack_api.scoring_functions",
    "llama_stack_api.prompts",
    "llama_stack_api.scoring",
    "llama_stack_api.safety",
    "llama_stack_api.vector_io",
    "llama_stack_api.connectors",
]

# 3. List every root-level .py file as a module.
# Because of package-dir above, these will be placed INSIDE the llama_stack_api folder.
# WARNING: Update this list when adding/removing root-level .py files
py-modules = [
    "llama_stack_api.datatypes",
    "llama_stack_api.openai_responses",
    "llama_stack_api.rag_tool",
    "llama_stack_api.resource",
    "llama_stack_api.router_utils",
    "llama_stack_api.schema_utils",
    "llama_stack_api.tools",
    "llama_stack_api.vector_stores",
    "llama_stack_api.version",
    "llama_stack_api.validators",
]

[tool.setuptools.package-data]
llama_stack_api = ["py.typed", "**/*.json", "**/*.yaml"]

[tool.setuptools_scm]
root = "../.."
fallback_version = "0.4.4.dev0"

[tool.ruff]
line-length = 120

[tool.ruff.lint]
select = [
    "UP",      # pyupgrade
    "B",       # flake8-bugbear
    "B9",      # flake8-bugbear subset
    "C",       # comprehensions
    "E",       # pycodestyle
    "F",       # Pyflakes
    "N",       # Naming
    "W",       # Warnings
    "DTZ",     # datetime rules
    "I",       # isort (imports order)
    "RUF001",  # Checks for ambiguous Unicode characters in strings
    "RUF002",  # Checks for ambiguous Unicode characters in docstrings
    "RUF003",  # Checks for ambiguous Unicode characters in comments
    "PLC2401", # Checks for the use of non-ASCII characters in variable names
]
ignore = [
    # The following ignores are desired by the project maintainers.
    "E402",   # Module level import not at top of file
    "E501",   # Line too long
    "F405",   # Maybe undefined or defined from star import
    "C408",   # Ignored because we like the dict keyword argument syntax
    "N812",   # Ignored because import torch.nn.functional as F is PyTorch convention

    # These are the additional ones we started ignoring after moving to ruff. We should look into each one of them later.
    "C901",   # Complexity of the function is too high
]
unfixable = [
    "PLE2515",
] # Do not fix this automatically since ruff will replace the zero-width space with \u200b - let's do it manually

[tool.ruff.lint.per-file-ignores]
"llama_stack_api/apis/**/__init__.py" = ["F403"]

[tool.ruff.lint.pep8-naming]
classmethod-decorators = ["classmethod", "pydantic.field_validator"]
