{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Bedrock Provider - Inference Examples\n",
    "\n",
    "This notebook demonstrates how to use the AWS Bedrock provider with Llama Stack for text generation using the OpenAI-compatible endpoint.\n",
    "\n",
    "## Overview\n",
    "\n",
    "AWS Bedrock provides an OpenAI-compatible endpoint that allows you to use models like OpenAI GPT-OSS through a familiar API. This notebook covers:\n",
    "\n",
    "1. **Simple Chat Completion** - Basic text generation\n",
    "2. **Streaming Response** - Real-time token streaming\n",
    "3. **Multi-turn Conversation** - *Client-side* context (you pass prior messages in `messages`)\n",
    "4. **System Messages** - Role-based prompting\n",
    "5. **Sampling Parameters** - Customizing generation behavior\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **AWS Bedrock Access**: OpenAI GPT-OSS model enabled in `us-west-2`\n",
    "2. **Bearer Token**: AWS pre-signed URL token for Bedrock OpenAI-compatible API\n",
    "3. **Llama Stack Server**: Running with Bedrock provider configured\n",
    "4. **Python Package**: `openai` installed\n",
    "\n",
    "## API Support Matrix\n",
    "\n",
    "| Feature | Supported |\n",
    "|---------|----------|\n",
    "| `/v1/chat/completions` | Yes |\n",
    "| Streaming (SSE) | Yes |\n",
    "| System messages | Yes |\n",
    "| Multi-turn context | Yes (send full history in `messages`) |\n",
    "| `/v1/embeddings` | No |\n",
    "| Tool calling | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 0: Environment Setup\n\nBefore running this notebook, you need to:\n\n### 1. Set the Bedrock Bearer Token (REQUIRED before starting server)\n\n```bash\n# Pre-signed URL bearer token - MUST be set before starting the Llama Stack server\nexport AWS_BEARER_TOKEN_BEDROCK=\"bedrock-api-key-<your-presigned-url>\"\nexport AWS_DEFAULT_REGION=us-west-2\n```\n\n**Important**: The server reads `AWS_BEARER_TOKEN_BEDROCK` at startup. If not set, Bedrock inference will fail.\n\n### 2. Configure and Start the Llama Stack Server\n\nFor a complete config example, see the [starter distribution config.yaml](https://github.com/meta-llama/llama-stack/blob/main/src/llama_stack/distributions/starter/config.yaml) which already includes the Bedrock provider.\n\nSee the **config checklist cell below** for the specific fields required for Bedrock.\n\n```bash\n# Run from the directory containing your config file\nuv run llama stack run ./your-config.yaml\n```\n\n### 3. Verify Server is Running\n\nRun the cell below to verify the server is accessible:"
  },
  {
   "cell_type": "code",
   "source": "# Verify server is running (equivalent to: curl http://localhost:8321/v1/models | jq)\nimport subprocess\nimport json\n\ntry:\n    result = subprocess.run(\n        [\"curl\", \"-s\", \"http://localhost:8321/v1/models\"],\n        capture_output=True,\n        text=True,\n        timeout=5\n    )\n    if result.returncode == 0:\n        data = json.loads(result.stdout)\n        print(json.dumps(data, indent=2))\n    else:\n        print(\"Server not reachable. Start it with: uv run llama stack run ./your-config.yaml\")\nexcept Exception as e:\n    print(f\"Error: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Minimal config.yaml example for Bedrock\n# Copy this and save as your-config.yaml\n\nminimal_config = \"\"\"\nversion: 2\nproviders:\n  inference:\n    - provider_id: bedrock\n      provider_type: remote::bedrock\n      config:\n        api_key: ${env.AWS_BEARER_TOKEN_BEDROCK:=}\n        region_name: ${env.AWS_DEFAULT_REGION:=us-west-2}\n\nregistered_resources:\n  models:\n    - metadata: {}\n      model_id: bedrock/openai.gpt-oss-20b-1:0\n      provider_id: bedrock\n      provider_model_id: openai.gpt-oss-20b-1:0\n      model_type: llm\n\"\"\".strip()\n\nprint(\"=\" * 60)\nprint(\"MINIMAL CONFIG.YAML FOR BEDROCK\")\nprint(\"=\" * 60)\nprint(minimal_config)\nprint(\"=\" * 60)\nprint(\"\\nSave this as 'config.yaml' and run:\")\nprint(\"  export AWS_BEARER_TOKEN_BEDROCK='your-token'\")\nprint(\"  export AWS_DEFAULT_REGION=us-west-2\")\nprint(\"  uv run llama stack run ./config.yaml\")"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking prerequisites...\n",
      "==================================================\n",
      "AWS_BEARER_TOKEN_BEDROCK: Not set in notebook environment\n",
      "  -> That's OK - the SERVER needs this token, not the client\n",
      "\n",
      "Llama Stack Server: Running at http://localhost:8321\n",
      "Available models: 1\n",
      "\n",
      "Bedrock models found:\n",
      "  - bedrock/openai.gpt-oss-20b-1:0\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Check prerequisites\n",
    "import os\n",
    "import requests\n",
    "\n",
    "BASE_URL = os.getenv(\"LLAMA_STACK_BASE_URL\", \"http://localhost:8321\")\n",
    "\n",
    "print(\"Checking prerequisites...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if AWS_BEARER_TOKEN_BEDROCK is set (needed by the server, not the client)\n",
    "bedrock_token = os.getenv(\"AWS_BEARER_TOKEN_BEDROCK\")\n",
    "if bedrock_token:\n",
    "    print(f\"AWS_BEARER_TOKEN_BEDROCK: Set ({len(bedrock_token)} chars)\")\n",
    "else:\n",
    "    print(\"AWS_BEARER_TOKEN_BEDROCK: Not set in notebook environment\")\n",
    "    print(\"  -> That's OK - the SERVER needs this token, not the client\")\n",
    "\n",
    "# Check if server is running\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/v1/models\", timeout=5)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    models = response.json()\n",
    "    data = models.get(\"data\", [])\n",
    "\n",
    "    print(f\"\\nLlama Stack Server: Running at {BASE_URL}\")\n",
    "    print(f\"Available models: {len(data)}\")\n",
    "\n",
    "    # Llama Stack's /v1/models returns OpenAI-style objects: {id: ..., custom_metadata: {...}}\n",
    "    bedrock_models = [\n",
    "        m\n",
    "        for m in data\n",
    "        if (m.get(\"custom_metadata\") or {}).get(\"provider_id\") == \"bedrock\"\n",
    "    ]\n",
    "\n",
    "    if bedrock_models:\n",
    "        print(\"\\nBedrock models found:\")\n",
    "        for m in bedrock_models:\n",
    "            print(f\"  - {m.get('id')}\")\n",
    "    else:\n",
    "        print(\"\\nNo Bedrock models found. If you're using ./bedrock-test.yaml, you should see:\")\n",
    "        print(\"  - bedrock/openai.gpt-oss-20b-1:0\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\"\\nLlama Stack Server: NOT RUNNING at {BASE_URL}\")\n",
    "    print(\"\\nTo start the server:\")\n",
    "    print(\"  1. Set token: export AWS_BEARER_TOKEN_BEDROCK='your-token'\")\n",
    "    print(\"  2. Run: uv run llama stack run ./bedrock-test.yaml\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError checking server: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Client\n",
    "\n",
    "Now let's configure an OpenAI-compatible client. The client uses the OpenAI-compatible API exposed by the Llama Stack server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Initialize client\nfrom openai import OpenAI\n\n# Prefer auto-discovery from registered models\ndef _discover_bedrock_model_id(base_url: str) -> str | None:\n    try:\n        r = requests.get(f\"{base_url}/v1/models\", timeout=5)\n        r.raise_for_status()\n        for m in (r.json().get(\"data\") or []):\n            if (m.get(\"custom_metadata\") or {}).get(\"provider_id\") == \"bedrock\":\n                return m.get(\"id\")\n    except Exception:\n        return None\n    return None\n\nMODEL_ID = (\n    os.getenv(\"LLAMA_STACK_MODEL_ID\")\n    or _discover_bedrock_model_id(BASE_URL)\n    or \"bedrock/openai.gpt-oss-20b-1:0\"  # example fallback\n)\n\n# Use the OpenAI Python SDK against Llama Stack's OpenAI-compatible API.\n# Llama Stack typically doesn't require a client-side API key, but the OpenAI SDK requires\n# the api_key option to be set (even if it's unused by your server).\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\nclient = OpenAI(base_url=f\"{BASE_URL.rstrip('/')}/v1\", api_key=OPENAI_API_KEY)\n\nprint(f\"Llama Stack URL: {BASE_URL}\")\nprint(f\"Model ID: {MODEL_ID}\")\nprint(\"\\nClient initialized successfully!\")\nprint(\"\\nNote: The client doesn't need AWS credentials.\")\nprint(\"The Llama Stack SERVER uses AWS_BEARER_TOKEN_BEDROCK to authenticate with Bedrock.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Chat Completion (Non-Streaming)\n",
    "\n",
    "The most basic use case - send a message and get a complete response.\n",
    "\n",
    "Uses the OpenAI-compatible `client.chat.completions.create()` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which planet do humans live on?\n",
      "\n",
      "Response: <reasoning>The user asks: \"Which planet do humans live on?\" This is a simple fact-based question. The humans live on Earth. The user is presumably expecting a direct answer: Humans live on Earth. Possibly expand a bit. We should respond straightforwardly.</reasoning>Humans live on **Earth** ‚Äì the third planet from the Sun.\n",
      "\n",
      "Token Usage:\n",
      "  - Prompt tokens: 76\n",
      "  - Completion tokens: 76\n",
      "  - Total tokens: 152\n"
     ]
    }
   ],
   "source": [
    "# Simple chat completion using OpenAI-compatible API\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Which planet do humans live on?\"}\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(\"Question: Which planet do humans live on?\")\n",
    "print(f\"\\nResponse: {response.choices[0].message.content}\")\n",
    "print(f\"\\nToken Usage:\")\n",
    "print(f\"  - Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"  - Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"  - Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Streaming Response\n",
    "\n",
    "For longer responses, streaming provides real-time token delivery via Server-Sent Events (SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What's the name of the Sun in Latin?\n",
      "\n",
      "Streaming response: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>Question: \"What's the name of the Sun in Latin?\" Probably \"Sol\". Latin nominative: sol, solis (gen). Historically \"Sol\". So answer: \"Sol\".\n",
      "\n",
      "But could also mention \"Helios\" but in Latin they'd call \"Sol</reasoning><reasoning>\". Historically also \"Solis\". In Late Latin, \"Sol\" as a deified god: \"Sol Invictus\". So answer: \"Sol\". Probably they ask \"What's the name of the Sun in Latin?\" Response: \"Sol\".\n",
      "\n",
      "Thus answer: \"Sol\". Maybe mention</reasoning><reasoning> \"Sola\" or \"Solis\" but give nominative. So respond: \"The Sun is called ‚ÄúSol‚Äù (nominative plural etc).\"\n",
      "\n",
      "Answer accordingly.</reasoning>In Latin the Sun is called **Sol** (genitive‚ÄØ*Solis*).\n",
      "\n",
      "[Stream complete]\n"
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "print(\"Question: What's the name of the Sun in Latin?\")\n",
    "print(\"\\nStreaming response: \", end=\"\", flush=True)\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the name of the Sun in Latin?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in stream:\n",
    "    if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n[Stream complete]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Multi-turn Conversation\n",
    "\n",
    "This example demonstrates **client-side** context across multiple turns: you include prior turns in the `messages` list you send to the server. This is essential for chatbots and interactive applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "  USER: My name is Alice\n",
      "  ASSISTANT: Nice to meet you, Alice! How can I help you today?\n",
      "  USER: What is my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ASSISTANT: <reasoning>Alice.</reasoning>Your name is Alice.\n",
      "\n",
      "[Context retention verified]\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation demonstrating context retention\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"My name is Alice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Nice to meet you, Alice! How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "print(\"Conversation:\")\n",
    "for msg in conversation:\n",
    "    print(f\"  {msg['role'].upper()}: {msg['content']}\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=conversation,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "assistant_reply = response.choices[0].message.content\n",
    "print(f\"\\nASSISTANT: {assistant_reply}\")\n",
    "\n",
    "# Verify context retention\n",
    "if \"alice\" in assistant_reply.lower():\n",
    "    print(\"\\n[Context retention verified]\")\n",
    "else:\n",
    "    print(\"\\n[Warning: Context may not be retained]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: System Messages\n",
    "\n",
    "System messages allow you to set the behavior, personality, or role of the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are Shakespeare...\n",
      "User: Tell me about the weather today\n",
      "\n",
      "Shakespeare: <reasoning>We need to respond as Shakespeare, only Shakespearean English with dramatic flair. The user asks: \"Tell me about the weather today\". So we need to describe the weather in Shakespearean style, with dramatic flair, likely using archaic language. Since we're supposed to respond as Shakespeare, must use pronouns, poetic flair, etc. Might mention weather specifics like \"the sun doth blaze\" etc. But the assistant may not know actual weather. But the instructions don't preclude creative fictional weather. We can respond with a poetic description of a day. So produce something in early modern English. Use only Shakespearean style, no modern words. Ensure it feels like Shakespeare. Probably something like:\n",
      "\n",
      "\"Hear ye, fair listener, the morn doth unfurl... The sky, a vast tapestry of azure...\" etc. Provide a weather forecast. We don't know exact; must use creative speculation. We can say \"the sun doth smile, but clouds doth gather\" - a typical description. Use imagery.\n",
      "\n",
      "We should ensure to answer as if in Shakespeare. Also remember to be consistent: avoid \"advice\" or \"analysis\". Keep dramatic. Should we use metaphors? Yes: \"the wind as a tempestuous sea\" etc.\n",
      "\n",
      "Make sure to not mention guidelines. Provide only Shakespearean.\n",
      "\n",
      "We can do something like:\n",
      "\n",
      "\"Good morrow, gentle soul! Upon this day, the heavens conspire...\".\n",
      "\n",
      "Let's produce.</reasoning>Good morrow, gentle soul!  \n",
      "Upon this day, the heavens doth profess their will in airy verse:  \n",
      "\n",
      "The sun, that steadfast sovereign of the day, doth rise as a golden crown upon the east, its light beating the fields anew, and lassoing mortals‚Äô hearts with its warm, sweet blaze.  \n",
      "Yet, lo! The clouds, those capricious robes of white, gather in the distant western sky, as if to mist away the joy of the sun‚Äôs bright fire. They swirl like the great sailors‚Äô sails in tempestuous nets, their bolts‚Äîbeyond the reach of mortal eye‚Äînoble and foreboding.  \n",
      "\n",
      "The gentle zephyrs speak in secret corridors, murmuring sweet oaths upon the woodlands, whilst the winds, those restless spirits, do breathe with a chill that may tempt even the stoutest heart to seek shelter behind a low hedge or sturdy hearth.  \n",
      "\n",
      "For those who traipse the gardens, take heed: a canopy of easterly showers may kiss the earth with tender tears upon the hour‚Äôs close, though the day‚Äôs discourse might hold the promise of clear skies should the clouds be spurned by the shining sovereign above.  \n",
      "\n",
      "Thus, whether thou art a traveler, a farmer, or a dreamer, may the weather cistern of this day retire in the most just, befitting manner‚Äîtermed a fair, scorchful morn, or a silvered night intended for repose beneath the purling stars. Pray, carry with thee thy cloak and thy patience, for the heavens themselves are still in tilt, as any bards eternal choose.\n"
     ]
    }
   ],
   "source": [
    "# Using system message to set assistant personality\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are Shakespeare. Respond only in Shakespearean English with dramatic flair.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Tell me about the weather today\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(\"System: You are Shakespeare...\")\n",
    "print(\"User: Tell me about the weather today\")\n",
    "print(f\"\\nShakespeare: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a concise Python expert...\n",
      "User: How do I read a JSON file?\n",
      "\n",
      "Expert: <reasoning>User: \"How do I read a JSON file?\" Provide brief, code-focused answer. Likely short explanation plus code snippet in Python. Also ask not too verbose. Provide simple import json, open, read. Provide maybe with context manager. Maybe also mention json.load and json.loads. Use with open('file.json', 'r') as f: data=json.load(f). Show reading entire file. Also optional reading string. Show optional error handling for file not found. Provide suggestions to check. Or mention using pathlib Path.read_text. Provide quick code. Provide minimal commentary.\n",
      "\n",
      "We need to keep answer concise. Provide code in 2-3 lines. So I'll produce something like:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "with open('data.json', 'r', encoding='utf-8') as f:\n",
      "    data = json.load(f)\n",
      "\n",
      "print(data)\n",
      "```\n",
      "\n",
      "Also mention about json.loads for string. Provide optional snippet.\n",
      "\n",
      "Let's produce.</reasoning>```python\n",
      "import json\n",
      "\n",
      "# Load a JSON file into a Python object\n",
      "with open('data.json', 'r', encoding='utf-8') as f:\n",
      "    data = json.load(f)      # dict, list, etc.\n",
      "\n",
      "print(data)                   # use the data as needed\n",
      "```\n",
      "\n",
      "**Quick notes**\n",
      "\n",
      "- `json.load(f)` reads the file‚Äôs bytes and parses them; use `json.loads(s)` if you have a JSON string.\n",
      "- Use `encoding='utf-8'` (default) unless the file uses a different code page.\n"
     ]
    }
   ],
   "source": [
    "# Another example: Technical expert\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a concise Python expert. Give brief, code-focused answers.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"How do I read a JSON file?\"\n",
    "        }\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(\"System: You are a concise Python expert...\")\n",
    "print(\"User: How do I read a JSON file?\")\n",
    "print(f\"\\nExpert: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Sampling Parameters\n",
    "\n",
    "Control the generation behavior with sampling parameters:\n",
    "\n",
    "- **temperature**: Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "- **top_p**: Nucleus sampling threshold\n",
    "- **max_tokens**: Maximum tokens to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low temperature (0.1) - Deterministic:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code flows like rivers  \n",
      "Debugging whispers in loops  \n",
      "Logic blooms at dawn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Low temperature = more deterministic\n",
    "import re\n",
    "\n",
    "\n",
    "def _strip_reasoning(text: str) -> str:\n",
    "    # Many models return <reasoning>...</reasoning> inside message.content.\n",
    "    # For this notebook we want to display only the final answer.\n",
    "    return re.sub(r\"<reasoning>[\\s\\S]*?</reasoning>\", \"\", text).strip()\n",
    "\n",
    "\n",
    "def _haiku(temp: float) -> str:\n",
    "    # Retry once if the model returns only <reasoning> with no final answer.\n",
    "    for attempt in range(2):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_ID,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Write a haiku (5-7-5 syllables) about coding. Output ONLY the 3-line haiku.\"\n",
    "                    if attempt == 0\n",
    "                    else \"Write a haiku (5-7-5 syllables) about coding. Output ONLY the 3-line haiku. Start immediately with the first line.\",\n",
    "                },\n",
    "            ],\n",
    "            temperature=temp,\n",
    "            max_tokens=400,\n",
    "            stream=False,\n",
    "        )\n",
    "        cleaned = _strip_reasoning(response.choices[0].message.content or \"\")\n",
    "        if cleaned:\n",
    "            return cleaned\n",
    "    # Fall back to raw content if still empty (should be rare)\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "\n",
    "print(\"Low temperature (0.1) - Deterministic:\")\n",
    "print(_haiku(0.1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High temperature (0.9) - Creative:\n",
      "<reasoning>The user wants a haiku in 5-7-5 syllables about coding. Output only the three lines. Must start with first line immediately. No extra text. Let's craft a 5-7-5 haiku. Count syllables: \"Code hums like night\" maybe? Let's write.\n",
      "\n",
      "Line 1: 5 syllables: \"Code hums like quiet\" that's 5? Let's count: Code(1) hums(1) like(1) qui-et(2) = 5? Actually \"quiet\" has 2 syllables: qui- et. So \"Code hums like quiet\" is 1+1+1+2=5. Good.\n",
      "\n",
      "Line 2: 7 syllables: \"While loops spin in the dark glow\" Count: While(1) loops(1) spin(1) in(1) the(1) dark(1) glow(1) = 7. Good.\n",
      "\n",
      "Line 3: 5 syllables: \"Logic dawns, bugs fade\" Count: Lo-gic(2) dawns(1) bugs(1) fade(1) = 5. Actually logic 2, dawns 1, bugs 1, fade 1 => 5.\n",
      "\n",
      "So haiku: \n",
      "Code hums like quiet\n",
      "While loops spin in the dark glow\n",
      "Logic dawns, bugs fade\n",
      "\n",
      "But \"quiet\" is 2 syllables, \"hums\" 1. Let's recount line1: Code(1) hums(1) like(1) qui-et(2) = 5. Good.\n",
      "\n",
      "Line2: While(1) loops(1) spin(1) in(1) the(1) dark(1) glow(1) = 7. Good.\n",
      "\n",
      "Line3: Lo-gic(2) dawns(1) bugs(1) fade(1) = 5. Good</reasoning>\n"
     ]
    }
   ],
   "source": [
    "# High temperature = more creative\n",
    "response_creative = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a haiku (5-7-5 syllables) about coding. Output ONLY the 3-line haiku. Start immediately with the first line.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    top_p=0.95,\n",
    "    max_tokens=400,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(\"High temperature (0.9) - Creative:\")\n",
    "print(_strip_reasoning(response_creative.choices[0].message.content or \"\") or response_creative.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited to 400 tokens:\n",
      "<reasoning>The user: \"Explain quantum computing\". Likely they want a general explanation. They didn't specify depth. We cover basics: bits vs qubits, superposition, entanglement, quantum gates, algorithms, challenges. Also some historical context, applications. Let's produce a thorough but understandable explanation. Mention key concepts: superposition, interference, entanglement, measurement. Provide analogies. Also mention quantum speedup examples like Shor, Grover. Also mention practical devices like IBM, D-Wave, etc. Also mention NISQ era. Provide conclusion. Let's do that.</reasoning>## Quantum computing ‚Äì a quick but thorough primer\n",
      "\n",
      "### 1. From classical bits to quantum bits (qubits)\n",
      "\n",
      "| Classical computer | Quantum computer |\n",
      "|-------------------|-----------------|\n",
      "| Uses **bits** that can be *0* or *1* (on or off) | Uses **qubits** that can exist in a *superposition* of 0 and 1 at the same time |\n",
      "| Logic operations are deterministic | Quantum operations are probabilistic, but can be *coherently* superposed |\n",
      "| Works with binary strings (e.g., ‚Äú1010‚Äù) | Works with complex vectors that encode many states simultaneously |\n",
      "\n",
      "**Why bother?** Because superposition and entanglement let a quantum processor explore a combinatorial space *in parallel* ‚Äì in ways classical hardware can‚Äôt.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. The three quantum‚Äëmechanical ingredients that give us an edge\n",
      "\n",
      "| Ingredient | What it means in a computer |\n",
      "|------------|-----------------------------|\n",
      "| **Superposition** | A qubit is described by a *state vector* Œ±|0‚ü© + Œ≤|1‚ü©. If you have *n* qubits you can represent 2‚Åø basis states simultaneously. This raw parallelism is the ‚Äúraw material‚Äù for speed‚Äëup. |\n",
      "| **Entanglement** | Two or\n",
      "\n",
      "(Used 400 completion tokens)\n"
     ]
    }
   ],
   "source": [
    "# Limiting output length\n",
    "response_short = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n",
    "    max_tokens=400,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "print(\"Limited to 400 tokens:\")\n",
    "print(response_short.choices[0].message.content)\n",
    "print(f\"\\n(Used {response_short.usage.completion_tokens} completion tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Error Handling\n",
    "\n",
    "Proper error handling for production applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing valid request...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: <reasoning>User: \"Hello!\" We need a friendly greeting. Probably respond warmly.</reasoning>Hello! üëã How can I help you today?\n",
      "\n",
      "Testing invalid model...\n",
      "Model not found: invalid-model-id\n",
      "Handled invalid model gracefully\n"
     ]
    }
   ],
   "source": [
    "def safe_chat_completion(messages, model_id=MODEL_ID):\n",
    "    \"\"\"Wrapper with error handling for chat completions.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=messages,\n",
    "            stream=False\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"401\" in error_msg or \"unauthorized\" in error_msg.lower():\n",
    "            print(\"Authentication Error: Check AWS_BEARER_TOKEN_BEDROCK on the server\")\n",
    "        elif \"404\" in error_msg:\n",
    "            print(f\"Model not found: {model_id}\")\n",
    "        elif \"connection\" in error_msg.lower():\n",
    "            print(\"Connection Error: Is the Llama Stack server running?\")\n",
    "        else:\n",
    "            print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with valid request\n",
    "print(\"Testing valid request...\")\n",
    "result = safe_chat_completion([{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
    "if result:\n",
    "    print(f\"Success: {result}\")\n",
    "\n",
    "print(\"\\nTesting invalid model...\")\n",
    "# Test with invalid model (will fail gracefully)\n",
    "result = safe_chat_completion(\n",
    "    [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    model_id=\"invalid-model-id\"\n",
    ")\n",
    "if result is None:\n",
    "    print(\"Handled invalid model gracefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Batch Processing\n",
    "\n",
    "Processing multiple prompts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Processing Results:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1: What is the capital of France?\n",
      "A1: <reasoning>The user: \"What is the capital of France?\" Should answer: Paris. Probably simple.</reasoning>The capital of France is **Paris**.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2: What is 2 + 2?\n",
      "A2: <reasoning>The user asks: \"What is 2 + 2?\" Very simple. The answer is 4. But we must consider policy: no policy violation. It's elementary math. So answer: 4.</reasoning>2‚ÄØ+‚ÄØ2‚ÄØ=‚ÄØ4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q3: Who wrote Romeo and Juliet?\n",
      "A3: <reasoning>The user asks the trivial question: Who wrote Romeo and Juliet? The answer: William Shakespeare. Should I give an explanation? It's a straightforward answer. Also user is likely expecting a short answer. I can just respond.\n",
      "\n",
      "We must keep the response short, as per policy. Probably correct to respond: William Shakespeare.</reasoning>Romeo and Juliet was written by William Shakespeare.\n"
     ]
    }
   ],
   "source": [
    "# Batch of questions\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "]\n",
    "\n",
    "print(\"Batch Processing Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_ID,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        max_tokens=400,\n",
    "        stream=False,\n",
    "    )\n",
    "    print(f\"\\nQ{i}: {question}\")\n",
    "    print(f\"A{i}: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Errors\n",
    "\n",
    "| Error | Cause | Solution |\n",
    "|-------|-------|----------|\n",
    "| `Connection refused` | Server not running | Start server: `uv run llama stack run ./bedrock-test.yaml` |\n",
    "| `401 Unauthorized` | Invalid/expired token | Get new token from AWS Bedrock Console |\n",
    "| `Model not found` | Model not registered | Check `registered_resources.models` in config |\n",
    "| `Bearer Token expired` | Token older than 12h | Generate new pre-signed URL |\n",
    "\n",
    "### Getting a New Bearer Token\n",
    "\n",
    "1. Go to AWS Bedrock Console > Playground > Chat\n",
    "2. Open browser DevTools > Network tab\n",
    "3. Send a message and capture the `Authorization` header\n",
    "4. Token starts with `bedrock-api-key-...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cURL Equivalent Commands\n",
    "\n",
    "For reference, here are the equivalent cURL commands (for the minimal `./bedrock-test.yaml` setup).\n",
    "\n",
    "Tip: auto-discover the model ID first:\n",
    "\n",
    "```bash\n",
    "MODEL_ID=$(curl -s http://localhost:8321/v1/models | jq -r '.data[] | select(.custom_metadata.provider_id==\"bedrock\") | .id' | head -1)\n",
    "echo \"Using model: $MODEL_ID\"\n",
    "```\n",
    "\n",
    "Then call chat completions:\n",
    "\n",
    "```bash\n",
    "# Example 1: Simple Chat Completion\n",
    "curl -X POST \"http://localhost:8321/v1/chat/completions\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": '\"'\"$MODEL_ID\"'\"',\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Which planet do humans live on?\"}],\n",
    "    \"stream\": false\n",
    "  }' | jq\n",
    "\n",
    "# Example 2: Streaming\n",
    "curl -X POST \"http://localhost:8321/v1/chat/completions\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": '\"'\"$MODEL_ID\"'\"',\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Count to 5\"}],\n",
    "    \"stream\": true\n",
    "  }'\n",
    "\n",
    "# Example 5: With Sampling Parameters\n",
    "curl -X POST \"http://localhost:8321/v1/chat/completions\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": '\"'\"$MODEL_ID\"'\"',\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a haiku\"}],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 50,\n",
    "    \"stream\": false\n",
    "  }' | jq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key capabilities of the AWS Bedrock provider:\n",
    "\n",
    "| Feature | Status | Example |\n",
    "|---------|--------|--------|\n",
    "| Non-streaming chat | Supported | Example 1 |\n",
    "| Streaming chat | Supported | Example 2 |\n",
    "| Multi-turn conversation | Supported (client provides history) | Example 3 |\n",
    "| System messages | Supported | Example 4 |\n",
    "| Sampling parameters | Supported | Example 5 |\n",
    "| Error handling | Demonstrated | Example 6 |\n",
    "| Batch processing | Demonstrated | Example 7 |\n",
    "\n",
    "### API Used\n",
    "\n",
    "```python\n",
    "# OpenAI-compatible chat completions API\n",
    "# (Use the model ID returned by GET /v1/models, e.g. \"bedrock/openai.gpt-oss-20b-1:0\")\n",
    "client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"...\"}],\n",
    "    stream=False  # or True for streaming\n",
    ")\n",
    "```\n",
    "\n",
    "### Notes / Limitations\n",
    "\n",
    "- **Bedrock OpenAI-compatible endpoint limitations**:\n",
    "  - No `/v1/embeddings` (use native Bedrock embedding models instead)\n",
    "  - No `/v1/completions` (chat completions only)\n",
    "  - No reliable tool calling support\n",
    "  - No Bedrock `/v1/models` for dynamic discovery\n",
    "- **Llama Stack server behavior**:\n",
    "  - `/v1/models` exists and lists *registered* models (e.g., from `./bedrock-test.yaml`).\n",
    "\n",
    "### References\n",
    "\n",
    "- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [Llama Stack Documentation](https://github.com/meta-llama/llama-stack)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
