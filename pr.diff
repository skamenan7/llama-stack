diff --git a/client-sdks/stainless/openapi.yml b/client-sdks/stainless/openapi.yml
index e86bf6bc11..68c12c7ae2 100644
--- a/client-sdks/stainless/openapi.yml
+++ b/client-sdks/stainless/openapi.yml
@@ -1472,7 +1472,7 @@ paths:
     get:
       responses:
         '200':
-          description: A ListOpenAIResponseObject.
+          description: Successful Response
           content:
             application/json:
               schema:
@@ -1491,7 +1491,7 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: List Openai Responses
+      summary: List all responses.
       description: List all responses.
       operationId: list_openai_responses_v1_responses_get
       parameters:
@@ -1502,7 +1502,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: The ID of the last response to return.
           title: After
+        description: The ID of the last response to return.
       - name: limit
         in: query
         required: false
@@ -1510,8 +1512,10 @@ paths:
           anyOf:
           - type: integer
           - type: 'null'
+          description: The number of responses to return.
           default: 50
           title: Limit
+        description: The number of responses to return.
       - name: model
         in: query
         required: false
@@ -1519,7 +1523,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: The model to filter responses by.
           title: Model
+        description: The model to filter responses by.
       - name: order
         in: query
         required: false
@@ -1527,12 +1533,14 @@ paths:
           anyOf:
           - $ref: '#/components/schemas/Order'
           - type: 'null'
+          description: The order to sort responses by when sorted by created_at ('asc' or 'desc').
           default: desc
           title: Order
+        description: The order to sort responses by when sorted by created_at ('asc' or 'desc').
     post:
       responses:
         '200':
-          description: An OpenAIResponseObject.
+          description: An OpenAIResponseObject or a stream of OpenAIResponseObjectStream.
           content:
             application/json:
               schema:
@@ -1554,7 +1562,7 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: Create Openai Response
+      summary: Create a model response.
       description: Create a model response.
       operationId: create_openai_response_v1_responses_post
       requestBody:
@@ -1562,55 +1570,31 @@ paths:
         content:
           application/json:
             schema:
-              $ref: '#/components/schemas/CreateOpenaiResponseRequest'
-        x-llama-stack-extra-body-params:
-          guardrails:
-            $defs:
-              ResponseGuardrailSpec:
-                description: |-
-                  Specification for a guardrail to apply during response generation.
-
-                  :param type: The type/identifier of the guardrail.
-                properties:
-                  type:
-                    title: Type
-                    type: string
-                required:
-                - type
-                title: ResponseGuardrailSpec
-                type: object
-            anyOf:
-            - items:
-                anyOf:
-                - type: string
-                - $ref: '#/components/schemas/ResponseGuardrailSpec'
-              type: array
-            - type: 'null'
-            description: List of guardrails to apply during response generation. Guardrails provide safety and content moderation.
+              $ref: '#/components/schemas/CreateResponseRequest'
   /v1/responses/{response_id}:
     get:
       responses:
         '200':
-          description: An OpenAIResponseObject.
+          description: Successful Response
           content:
             application/json:
               schema:
                 $ref: '#/components/schemas/OpenAIResponseObject'
         '400':
-          description: Bad Request
           $ref: '#/components/responses/BadRequest400'
+          description: Bad Request
         '429':
-          description: Too Many Requests
           $ref: '#/components/responses/TooManyRequests429'
+          description: Too Many Requests
         '500':
-          description: Internal Server Error
           $ref: '#/components/responses/InternalServerError500'
+          description: Internal Server Error
         default:
-          description: Default Response
           $ref: '#/components/responses/DefaultError'
+          description: Default Response
       tags:
       - Agents
-      summary: Get Openai Response
+      summary: Get a model response.
       description: Get a model response.
       operationId: get_openai_response_v1_responses__response_id__get
       parameters:
@@ -1619,30 +1603,32 @@ paths:
         required: true
         schema:
           type: string
-        description: 'Path parameter: response_id'
+          description: The ID of the OpenAI response to retrieve.
+          title: Response Id
+        description: The ID of the OpenAI response to retrieve.
     delete:
       responses:
         '200':
-          description: An OpenAIDeleteResponseObject
+          description: Successful Response
           content:
             application/json:
               schema:
                 $ref: '#/components/schemas/OpenAIDeleteResponseObject'
         '400':
-          description: Bad Request
           $ref: '#/components/responses/BadRequest400'
+          description: Bad Request
         '429':
-          description: Too Many Requests
           $ref: '#/components/responses/TooManyRequests429'
+          description: Too Many Requests
         '500':
-          description: Internal Server Error
           $ref: '#/components/responses/InternalServerError500'
+          description: Internal Server Error
         default:
-          description: Default Response
           $ref: '#/components/responses/DefaultError'
+          description: Default Response
       tags:
       - Agents
-      summary: Delete Openai Response
+      summary: Delete a response.
       description: Delete a response.
       operationId: delete_openai_response_v1_responses__response_id__delete
       parameters:
@@ -1651,12 +1637,14 @@ paths:
         required: true
         schema:
           type: string
-        description: 'Path parameter: response_id'
+          description: The ID of the OpenAI response to delete.
+          title: Response Id
+        description: The ID of the OpenAI response to delete.
   /v1/responses/{response_id}/input_items:
     get:
       responses:
         '200':
-          description: An ListOpenAIResponseInputItem.
+          description: Successful Response
           content:
             application/json:
               schema:
@@ -1675,10 +1663,18 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: List Openai Response Input Items
+      summary: List input items.
       description: List input items.
       operationId: list_openai_response_input_items_v1_responses__response_id__input_items_get
       parameters:
+      - name: response_id
+        in: path
+        required: true
+        schema:
+          type: string
+          description: The ID of the response to retrieve input items for.
+          title: Response Id
+        description: The ID of the response to retrieve input items for.
       - name: after
         in: query
         required: false
@@ -1686,7 +1682,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: An item ID to list items after, used for pagination.
           title: After
+        description: An item ID to list items after, used for pagination.
       - name: before
         in: query
         required: false
@@ -1694,7 +1692,21 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: An item ID to list items before, used for pagination.
           title: Before
+        description: An item ID to list items before, used for pagination.
+      - name: include
+        in: query
+        required: false
+        schema:
+          anyOf:
+          - type: array
+            items:
+              $ref: '#/components/schemas/ResponseItemInclude'
+          - type: 'null'
+          description: Additional fields to include in the response.
+          title: Include
+        description: Additional fields to include in the response.
       - name: limit
         in: query
         required: false
@@ -1702,8 +1714,10 @@ paths:
           anyOf:
           - type: integer
           - type: 'null'
+          description: A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
           default: 20
           title: Limit
+        description: A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
       - name: order
         in: query
         required: false
@@ -1711,24 +1725,10 @@ paths:
           anyOf:
           - $ref: '#/components/schemas/Order'
           - type: 'null'
+          description: The order to return the input items in.
           default: desc
           title: Order
-      - name: response_id
-        in: path
-        required: true
-        schema:
-          type: string
-        description: 'Path parameter: response_id'
-      - name: include
-        in: query
-        required: false
-        schema:
-          anyOf:
-          - type: array
-            items:
-              type: string
-          - type: 'null'
-          title: Include
+        description: The order to return the input items in.
   /v1/safety/run-shield:
     post:
       responses:
@@ -7462,15 +7462,16 @@ components:
       title: OpenAIResponseUsage
       description: Usage information for OpenAI response.
     ResponseGuardrailSpec:
-      description: Specification for a guardrail to apply during response generation.
       properties:
         type:
-          title: Type
           type: string
+          title: Type
+      additionalProperties: false
+      type: object
       required:
       - type
       title: ResponseGuardrailSpec
-      type: object
+      description: Specification for a guardrail to apply during response generation.
     OpenAIResponseInputTool:
       discriminator:
         mapping:
@@ -7544,178 +7545,6 @@ components:
       - server_label
       title: OpenAIResponseInputToolMCP
       description: Model Context Protocol (MCP) tool configuration for OpenAI response inputs.
-    CreateOpenaiResponseRequest:
-      properties:
-        input:
-          anyOf:
-          - type: string
-          - items:
-              anyOf:
-              - oneOf:
-                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                  title: OpenAIResponseMessage-Input
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                  title: OpenAIResponseOutputMessageWebSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                  title: OpenAIResponseOutputMessageFileSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                  title: OpenAIResponseOutputMessageFunctionToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                  title: OpenAIResponseOutputMessageMCPCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                  title: OpenAIResponseOutputMessageMCPListTools
-                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                  title: OpenAIResponseMCPApprovalRequest
-                discriminator:
-                  propertyName: type
-                  mapping:
-                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                    message: '#/components/schemas/OpenAIResponseMessage-Input'
-                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                title: OpenAIResponseMessage-Input | ... (7 variants)
-              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
-                title: OpenAIResponseInputFunctionToolCallOutput
-              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
-                title: OpenAIResponseMCPApprovalResponse
-              - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                title: OpenAIResponseMessage-Input
-              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse | OpenAIResponseMessage-Input
-            type: array
-            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-        model:
-          type: string
-          title: Model
-        prompt:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponsePrompt'
-            title: OpenAIResponsePrompt
-          - type: 'null'
-          title: OpenAIResponsePrompt
-        instructions:
-          anyOf:
-          - type: string
-          - type: 'null'
-        parallel_tool_calls:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        previous_response_id:
-          anyOf:
-          - type: string
-          - type: 'null'
-        conversation:
-          anyOf:
-          - type: string
-          - type: 'null'
-        store:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        stream:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: false
-        temperature:
-          anyOf:
-          - type: number
-          - type: 'null'
-        text:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseText'
-            title: OpenAIResponseText
-          - type: 'null'
-          title: OpenAIResponseText
-        tool_choice:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
-            title: OpenAIResponseInputToolChoiceMode
-          - oneOf:
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-              title: OpenAIResponseInputToolChoiceAllowedTools
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-              title: OpenAIResponseInputToolChoiceFileSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-              title: OpenAIResponseInputToolChoiceWebSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-              title: OpenAIResponseInputToolChoiceFunctionTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-              title: OpenAIResponseInputToolChoiceMCPTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-              title: OpenAIResponseInputToolChoiceCustomTool
-            discriminator:
-              propertyName: type
-              mapping:
-                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
-          - type: 'null'
-          title: OpenAIResponseInputToolChoiceMode
-        tools:
-          anyOf:
-          - items:
-              oneOf:
-              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                title: OpenAIResponseInputToolWebSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                title: OpenAIResponseInputToolFileSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
-                title: OpenAIResponseInputToolFunction
-              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
-                title: OpenAIResponseInputToolMCP
-              discriminator:
-                propertyName: type
-                mapping:
-                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
-                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
-                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
-            type: array
-          - type: 'null'
-        include:
-          anyOf:
-          - items:
-              $ref: '#/components/schemas/ResponseItemInclude'
-            type: array
-          - type: 'null'
-        max_infer_iters:
-          anyOf:
-          - type: integer
-          - type: 'null'
-          default: 10
-        max_tool_calls:
-          anyOf:
-          - type: integer
-          - type: 'null'
-        metadata:
-          anyOf:
-          - additionalProperties:
-              type: string
-            type: object
-          - type: 'null'
-      type: object
-      required:
-      - input
-      - model
-      title: CreateOpenaiResponseRequest
     OpenAIResponseObject:
       properties:
         created_at:
@@ -12210,6 +12039,209 @@ components:
       - reasoning.encrypted_content
       title: ConversationItemInclude
       description: Specify additional output data to include in the model response.
+    CreateResponseRequest:
+      properties:
+        input:
+          anyOf:
+          - type: string
+          - items:
+              anyOf:
+              - oneOf:
+                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
+                  title: OpenAIResponseMessage-Input
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                  title: OpenAIResponseOutputMessageWebSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                  title: OpenAIResponseOutputMessageFileSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                  title: OpenAIResponseOutputMessageFunctionToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                  title: OpenAIResponseOutputMessageMCPCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                  title: OpenAIResponseOutputMessageMCPListTools
+                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                  title: OpenAIResponseMCPApprovalRequest
+                discriminator:
+                  propertyName: type
+                  mapping:
+                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                    message: '#/components/schemas/OpenAIResponseMessage-Input'
+                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                title: OpenAIResponseMessage-Input | ... (7 variants)
+              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
+                title: OpenAIResponseInputFunctionToolCallOutput
+              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
+                title: OpenAIResponseMCPApprovalResponse
+              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse
+            type: array
+            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse]
+          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse]
+          description: Input message(s) to create the response.
+        model:
+          type: string
+          title: Model
+          description: The underlying LLM used for completions.
+        prompt:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponsePrompt'
+            title: OpenAIResponsePrompt
+          - type: 'null'
+          description: Prompt object with ID, version, and variables.
+          title: OpenAIResponsePrompt
+        instructions:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Instructions to guide the model's behavior.
+        parallel_tool_calls:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to enable parallel tool calls.
+          default: true
+        previous_response_id:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a previous response to continue from.
+        conversation:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a conversation to add the response to.
+        store:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to store the response in the database.
+          default: true
+        stream:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to stream the response.
+          default: false
+        temperature:
+          anyOf:
+          - type: number
+            maximum: 2.0
+            minimum: 0.0
+          - type: 'null'
+          description: Sampling temperature.
+        text:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseText'
+            title: OpenAIResponseText
+          - type: 'null'
+          description: Configuration for text response generation.
+          title: OpenAIResponseText
+        tool_choice:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
+            title: OpenAIResponseInputToolChoiceMode
+          - oneOf:
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+              title: OpenAIResponseInputToolChoiceAllowedTools
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+              title: OpenAIResponseInputToolChoiceFileSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+              title: OpenAIResponseInputToolChoiceWebSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+              title: OpenAIResponseInputToolChoiceFunctionTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+              title: OpenAIResponseInputToolChoiceMCPTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+              title: OpenAIResponseInputToolChoiceCustomTool
+            discriminator:
+              propertyName: type
+              mapping:
+                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
+          - type: 'null'
+          title: OpenAIResponseInputToolChoiceMode
+          description: How the model should select which tool to call (if any).
+        tools:
+          anyOf:
+          - items:
+              oneOf:
+              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                title: OpenAIResponseInputToolWebSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                title: OpenAIResponseInputToolFileSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
+                title: OpenAIResponseInputToolFunction
+              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
+                title: OpenAIResponseInputToolMCP
+              discriminator:
+                propertyName: type
+                mapping:
+                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
+                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
+                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
+            type: array
+          - type: 'null'
+          description: List of tools available to the model.
+        include:
+          anyOf:
+          - items:
+              $ref: '#/components/schemas/ResponseItemInclude'
+            type: array
+          - type: 'null'
+          description: Additional fields to include in the response.
+        max_infer_iters:
+          anyOf:
+          - type: integer
+            minimum: 1.0
+          - type: 'null'
+          description: Maximum number of inference iterations.
+          default: 10
+        guardrails:
+          anyOf:
+          - items:
+              anyOf:
+              - type: string
+              - $ref: '#/components/schemas/ResponseGuardrailSpec'
+                title: ResponseGuardrailSpec
+              title: string | ResponseGuardrailSpec
+            type: array
+          - type: 'null'
+          description: List of guardrails to apply during response generation.
+        max_tool_calls:
+          anyOf:
+          - type: integer
+          - type: 'null'
+          description: Max number of total calls to built-in tools that can be processed in a response.
+        metadata:
+          anyOf:
+          - additionalProperties:
+              type: string
+            type: object
+          - type: 'null'
+          description: Dictionary of metadata key-value pairs to attach to the response.
+      additionalProperties: false
+      type: object
+      required:
+      - input
+      - model
+      title: CreateResponseRequest
+      description: Request model for creating a response.
     DatasetPurpose:
       type: string
       enum:
diff --git a/conftest.py b/conftest.py
new file mode 100644
index 0000000000..b70fecbdc7
--- /dev/null
+++ b/conftest.py
@@ -0,0 +1,38 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the terms described in the LICENSE file in
+# the root directory of this source tree.
+
+"""
+Root-level conftest.py for pytest plugin loading.
+
+In pytest 8.4+, pytest_plugins must be defined at the rootdir conftest.py level,
+not in subdirectory conftest files. This file dynamically loads the appropriate
+fixture plugins based on which tests are being collected.
+"""
+
+
+def pytest_configure(config):
+    """Dynamically import fixture plugins based on test collection paths."""
+    # Get the args to determine which tests are being run
+    args = config.invocation_params.args
+
+    # Check if we're running unit tests
+    running_unit = any("tests/unit" in str(arg) or "tests\\unit" in str(arg) for arg in args)
+    # Check if we're running integration tests
+    running_integration = any("tests/integration" in str(arg) or "tests\\integration" in str(arg) for arg in args)
+
+    # If no specific path given, check if collecting from root
+    if not args or args == (".",) or args == ("tests",) or args == ("tests/",):
+        running_unit = True
+        running_integration = True
+
+    # Import plugins dynamically
+    if running_unit:
+        config.pluginmanager.import_plugin("tests.unit.fixtures")
+        # Load shared fixtures from openai_responses test file (used by conversations tests)
+        config.pluginmanager.import_plugin("tests.unit.providers.agents.meta_reference.test_openai_responses")
+
+    if running_integration:
+        config.pluginmanager.import_plugin("tests.integration.fixtures.common")
diff --git a/docs/docs/providers/agents/index.mdx b/docs/docs/providers/agents/index.mdx
index 200a3b9caa..7053b50d59 100644
--- a/docs/docs/providers/agents/index.mdx
+++ b/docs/docs/providers/agents/index.mdx
@@ -1,8 +1,4 @@
 ---
-description: |
-  Agents
-
-      APIs for creating and interacting with agentic systems.
 sidebar_label: Agents
 title: Agents
 ---
@@ -11,8 +7,4 @@ title: Agents
 
 ## Overview
 
-Agents
-
-    APIs for creating and interacting with agentic systems.
-
 This section contains documentation for all available providers for the **agents** API.
diff --git a/docs/static/deprecated-llama-stack-spec.yaml b/docs/static/deprecated-llama-stack-spec.yaml
index d24cea68a4..6730de188c 100644
--- a/docs/static/deprecated-llama-stack-spec.yaml
+++ b/docs/static/deprecated-llama-stack-spec.yaml
@@ -4120,15 +4120,16 @@ components:
       title: OpenAIResponseUsage
       description: Usage information for OpenAI response.
     ResponseGuardrailSpec:
-      description: Specification for a guardrail to apply during response generation.
       properties:
         type:
-          title: Type
           type: string
+          title: Type
+      additionalProperties: false
+      type: object
       required:
       - type
       title: ResponseGuardrailSpec
-      type: object
+      description: Specification for a guardrail to apply during response generation.
     OpenAIResponseInputTool:
       discriminator:
         mapping:
@@ -4202,178 +4203,6 @@ components:
       - server_label
       title: OpenAIResponseInputToolMCP
       description: Model Context Protocol (MCP) tool configuration for OpenAI response inputs.
-    CreateOpenaiResponseRequest:
-      properties:
-        input:
-          anyOf:
-          - type: string
-          - items:
-              anyOf:
-              - oneOf:
-                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                  title: OpenAIResponseMessage-Input
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                  title: OpenAIResponseOutputMessageWebSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                  title: OpenAIResponseOutputMessageFileSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                  title: OpenAIResponseOutputMessageFunctionToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                  title: OpenAIResponseOutputMessageMCPCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                  title: OpenAIResponseOutputMessageMCPListTools
-                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                  title: OpenAIResponseMCPApprovalRequest
-                discriminator:
-                  propertyName: type
-                  mapping:
-                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                    message: '#/components/schemas/OpenAIResponseMessage-Input'
-                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                title: OpenAIResponseMessage-Input | ... (7 variants)
-              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
-                title: OpenAIResponseInputFunctionToolCallOutput
-              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
-                title: OpenAIResponseMCPApprovalResponse
-              - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                title: OpenAIResponseMessage-Input
-              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse | OpenAIResponseMessage-Input
-            type: array
-            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-        model:
-          type: string
-          title: Model
-        prompt:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponsePrompt'
-            title: OpenAIResponsePrompt
-          - type: 'null'
-          title: OpenAIResponsePrompt
-        instructions:
-          anyOf:
-          - type: string
-          - type: 'null'
-        parallel_tool_calls:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        previous_response_id:
-          anyOf:
-          - type: string
-          - type: 'null'
-        conversation:
-          anyOf:
-          - type: string
-          - type: 'null'
-        store:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        stream:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: false
-        temperature:
-          anyOf:
-          - type: number
-          - type: 'null'
-        text:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseText'
-            title: OpenAIResponseText
-          - type: 'null'
-          title: OpenAIResponseText
-        tool_choice:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
-            title: OpenAIResponseInputToolChoiceMode
-          - oneOf:
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-              title: OpenAIResponseInputToolChoiceAllowedTools
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-              title: OpenAIResponseInputToolChoiceFileSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-              title: OpenAIResponseInputToolChoiceWebSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-              title: OpenAIResponseInputToolChoiceFunctionTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-              title: OpenAIResponseInputToolChoiceMCPTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-              title: OpenAIResponseInputToolChoiceCustomTool
-            discriminator:
-              propertyName: type
-              mapping:
-                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
-          - type: 'null'
-          title: OpenAIResponseInputToolChoiceMode
-        tools:
-          anyOf:
-          - items:
-              oneOf:
-              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                title: OpenAIResponseInputToolWebSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                title: OpenAIResponseInputToolFileSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
-                title: OpenAIResponseInputToolFunction
-              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
-                title: OpenAIResponseInputToolMCP
-              discriminator:
-                propertyName: type
-                mapping:
-                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
-                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
-                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
-            type: array
-          - type: 'null'
-        include:
-          anyOf:
-          - items:
-              $ref: '#/components/schemas/ResponseItemInclude'
-            type: array
-          - type: 'null'
-        max_infer_iters:
-          anyOf:
-          - type: integer
-          - type: 'null'
-          default: 10
-        max_tool_calls:
-          anyOf:
-          - type: integer
-          - type: 'null'
-        metadata:
-          anyOf:
-          - additionalProperties:
-              type: string
-            type: object
-          - type: 'null'
-      type: object
-      required:
-      - input
-      - model
-      title: CreateOpenaiResponseRequest
     OpenAIResponseObject:
       properties:
         created_at:
@@ -8868,6 +8697,211 @@ components:
       - reasoning.encrypted_content
       title: ConversationItemInclude
       description: Specify additional output data to include in the model response.
+    CreateResponseRequest:
+      properties:
+        input:
+          anyOf:
+          - type: string
+          - items:
+              anyOf:
+              - oneOf:
+                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
+                  title: OpenAIResponseMessage-Input
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                  title: OpenAIResponseOutputMessageWebSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                  title: OpenAIResponseOutputMessageFileSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                  title: OpenAIResponseOutputMessageFunctionToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                  title: OpenAIResponseOutputMessageMCPCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                  title: OpenAIResponseOutputMessageMCPListTools
+                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                  title: OpenAIResponseMCPApprovalRequest
+                discriminator:
+                  propertyName: type
+                  mapping:
+                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                    message: '#/components/schemas/OpenAIResponseMessage-Input'
+                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                title: OpenAIResponseMessage-Input | ... (7 variants)
+              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
+                title: OpenAIResponseInputFunctionToolCallOutput
+              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
+                title: OpenAIResponseMCPApprovalResponse
+              - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
+                title: OpenAIResponseMessage-Input
+              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse | OpenAIResponseMessage-Input
+            type: array
+            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
+          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
+          description: Input message(s) to create the response.
+        model:
+          type: string
+          title: Model
+          description: The underlying LLM used for completions.
+        prompt:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponsePrompt'
+            title: OpenAIResponsePrompt
+          - type: 'null'
+          description: Prompt object with ID, version, and variables.
+          title: OpenAIResponsePrompt
+        instructions:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Instructions to guide the model's behavior.
+        parallel_tool_calls:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to enable parallel tool calls.
+          default: true
+        previous_response_id:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a previous response to continue from.
+        conversation:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a conversation to add the response to.
+        store:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to store the response in the database.
+          default: true
+        stream:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to stream the response.
+          default: false
+        temperature:
+          anyOf:
+          - type: number
+            maximum: 2.0
+            minimum: 0.0
+          - type: 'null'
+          description: Sampling temperature.
+        text:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseText'
+            title: OpenAIResponseText
+          - type: 'null'
+          description: Configuration for text response generation.
+          title: OpenAIResponseText
+        tool_choice:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
+            title: OpenAIResponseInputToolChoiceMode
+          - oneOf:
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+              title: OpenAIResponseInputToolChoiceAllowedTools
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+              title: OpenAIResponseInputToolChoiceFileSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+              title: OpenAIResponseInputToolChoiceWebSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+              title: OpenAIResponseInputToolChoiceFunctionTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+              title: OpenAIResponseInputToolChoiceMCPTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+              title: OpenAIResponseInputToolChoiceCustomTool
+            discriminator:
+              propertyName: type
+              mapping:
+                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
+          - type: 'null'
+          title: OpenAIResponseInputToolChoiceMode
+          description: How the model should select which tool to call (if any).
+        tools:
+          anyOf:
+          - items:
+              oneOf:
+              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                title: OpenAIResponseInputToolWebSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                title: OpenAIResponseInputToolFileSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
+                title: OpenAIResponseInputToolFunction
+              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
+                title: OpenAIResponseInputToolMCP
+              discriminator:
+                propertyName: type
+                mapping:
+                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
+                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
+                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
+            type: array
+          - type: 'null'
+          description: List of tools available to the model.
+        include:
+          anyOf:
+          - items:
+              $ref: '#/components/schemas/ResponseItemInclude'
+            type: array
+          - type: 'null'
+          description: Additional fields to include in the response.
+        max_infer_iters:
+          anyOf:
+          - type: integer
+            minimum: 1.0
+          - type: 'null'
+          description: Maximum number of inference iterations.
+          default: 10
+        guardrails:
+          anyOf:
+          - items:
+              anyOf:
+              - type: string
+              - $ref: '#/components/schemas/ResponseGuardrailSpec'
+                title: ResponseGuardrailSpec
+              title: string | ResponseGuardrailSpec
+            type: array
+          - type: 'null'
+          description: List of guardrails to apply during response generation.
+        max_tool_calls:
+          anyOf:
+          - type: integer
+          - type: 'null'
+          description: Max number of total calls to built-in tools that can be processed in a response.
+        metadata:
+          anyOf:
+          - additionalProperties:
+              type: string
+            type: object
+          - type: 'null'
+          description: Dictionary of metadata key-value pairs to attach to the response.
+      additionalProperties: false
+      type: object
+      required:
+      - input
+      - model
+      title: CreateResponseRequest
+      description: Request model for creating a response.
     DatasetPurpose:
       type: string
       enum:
diff --git a/docs/static/experimental-llama-stack-spec.yaml b/docs/static/experimental-llama-stack-spec.yaml
index 7330f2f75d..692ab7296d 100644
--- a/docs/static/experimental-llama-stack-spec.yaml
+++ b/docs/static/experimental-llama-stack-spec.yaml
@@ -4183,16 +4183,6 @@ components:
       - total_tokens
       title: OpenAIResponseUsage
       description: Usage information for OpenAI response.
-    ResponseGuardrailSpec:
-      description: Specification for a guardrail to apply during response generation.
-      properties:
-        type:
-          title: Type
-          type: string
-      required:
-      - type
-      title: ResponseGuardrailSpec
-      type: object
     OpenAIResponseInputTool:
       discriminator:
         mapping:
diff --git a/docs/static/llama-stack-spec.yaml b/docs/static/llama-stack-spec.yaml
index 7bf3c8b3b1..2ffc8b61ad 100644
--- a/docs/static/llama-stack-spec.yaml
+++ b/docs/static/llama-stack-spec.yaml
@@ -1407,7 +1407,7 @@ paths:
     get:
       responses:
         '200':
-          description: A ListOpenAIResponseObject.
+          description: Successful Response
           content:
             application/json:
               schema:
@@ -1426,7 +1426,7 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: List Openai Responses
+      summary: List all responses.
       description: List all responses.
       operationId: list_openai_responses_v1_responses_get
       parameters:
@@ -1437,7 +1437,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: The ID of the last response to return.
           title: After
+        description: The ID of the last response to return.
       - name: limit
         in: query
         required: false
@@ -1445,8 +1447,10 @@ paths:
           anyOf:
           - type: integer
           - type: 'null'
+          description: The number of responses to return.
           default: 50
           title: Limit
+        description: The number of responses to return.
       - name: model
         in: query
         required: false
@@ -1454,7 +1458,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: The model to filter responses by.
           title: Model
+        description: The model to filter responses by.
       - name: order
         in: query
         required: false
@@ -1462,12 +1468,14 @@ paths:
           anyOf:
           - $ref: '#/components/schemas/Order'
           - type: 'null'
+          description: The order to sort responses by when sorted by created_at ('asc' or 'desc').
           default: desc
           title: Order
+        description: The order to sort responses by when sorted by created_at ('asc' or 'desc').
     post:
       responses:
         '200':
-          description: An OpenAIResponseObject.
+          description: An OpenAIResponseObject or a stream of OpenAIResponseObjectStream.
           content:
             application/json:
               schema:
@@ -1489,7 +1497,7 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: Create Openai Response
+      summary: Create a model response.
       description: Create a model response.
       operationId: create_openai_response_v1_responses_post
       requestBody:
@@ -1497,55 +1505,31 @@ paths:
         content:
           application/json:
             schema:
-              $ref: '#/components/schemas/CreateOpenaiResponseRequest'
-        x-llama-stack-extra-body-params:
-          guardrails:
-            $defs:
-              ResponseGuardrailSpec:
-                description: |-
-                  Specification for a guardrail to apply during response generation.
-
-                  :param type: The type/identifier of the guardrail.
-                properties:
-                  type:
-                    title: Type
-                    type: string
-                required:
-                - type
-                title: ResponseGuardrailSpec
-                type: object
-            anyOf:
-            - items:
-                anyOf:
-                - type: string
-                - $ref: '#/components/schemas/ResponseGuardrailSpec'
-              type: array
-            - type: 'null'
-            description: List of guardrails to apply during response generation. Guardrails provide safety and content moderation.
+              $ref: '#/components/schemas/CreateResponseRequest'
   /v1/responses/{response_id}:
     get:
       responses:
         '200':
-          description: An OpenAIResponseObject.
+          description: Successful Response
           content:
             application/json:
               schema:
                 $ref: '#/components/schemas/OpenAIResponseObject'
         '400':
-          description: Bad Request
           $ref: '#/components/responses/BadRequest400'
+          description: Bad Request
         '429':
-          description: Too Many Requests
           $ref: '#/components/responses/TooManyRequests429'
+          description: Too Many Requests
         '500':
-          description: Internal Server Error
           $ref: '#/components/responses/InternalServerError500'
+          description: Internal Server Error
         default:
-          description: Default Response
           $ref: '#/components/responses/DefaultError'
+          description: Default Response
       tags:
       - Agents
-      summary: Get Openai Response
+      summary: Get a model response.
       description: Get a model response.
       operationId: get_openai_response_v1_responses__response_id__get
       parameters:
@@ -1554,30 +1538,32 @@ paths:
         required: true
         schema:
           type: string
-        description: 'Path parameter: response_id'
+          description: The ID of the OpenAI response to retrieve.
+          title: Response Id
+        description: The ID of the OpenAI response to retrieve.
     delete:
       responses:
         '200':
-          description: An OpenAIDeleteResponseObject
+          description: Successful Response
           content:
             application/json:
               schema:
                 $ref: '#/components/schemas/OpenAIDeleteResponseObject'
         '400':
-          description: Bad Request
           $ref: '#/components/responses/BadRequest400'
+          description: Bad Request
         '429':
-          description: Too Many Requests
           $ref: '#/components/responses/TooManyRequests429'
+          description: Too Many Requests
         '500':
-          description: Internal Server Error
           $ref: '#/components/responses/InternalServerError500'
+          description: Internal Server Error
         default:
-          description: Default Response
           $ref: '#/components/responses/DefaultError'
+          description: Default Response
       tags:
       - Agents
-      summary: Delete Openai Response
+      summary: Delete a response.
       description: Delete a response.
       operationId: delete_openai_response_v1_responses__response_id__delete
       parameters:
@@ -1586,12 +1572,14 @@ paths:
         required: true
         schema:
           type: string
-        description: 'Path parameter: response_id'
+          description: The ID of the OpenAI response to delete.
+          title: Response Id
+        description: The ID of the OpenAI response to delete.
   /v1/responses/{response_id}/input_items:
     get:
       responses:
         '200':
-          description: An ListOpenAIResponseInputItem.
+          description: Successful Response
           content:
             application/json:
               schema:
@@ -1610,10 +1598,18 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: List Openai Response Input Items
+      summary: List input items.
       description: List input items.
       operationId: list_openai_response_input_items_v1_responses__response_id__input_items_get
       parameters:
+      - name: response_id
+        in: path
+        required: true
+        schema:
+          type: string
+          description: The ID of the response to retrieve input items for.
+          title: Response Id
+        description: The ID of the response to retrieve input items for.
       - name: after
         in: query
         required: false
@@ -1621,7 +1617,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: An item ID to list items after, used for pagination.
           title: After
+        description: An item ID to list items after, used for pagination.
       - name: before
         in: query
         required: false
@@ -1629,7 +1627,21 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: An item ID to list items before, used for pagination.
           title: Before
+        description: An item ID to list items before, used for pagination.
+      - name: include
+        in: query
+        required: false
+        schema:
+          anyOf:
+          - type: array
+            items:
+              $ref: '#/components/schemas/ResponseItemInclude'
+          - type: 'null'
+          description: Additional fields to include in the response.
+          title: Include
+        description: Additional fields to include in the response.
       - name: limit
         in: query
         required: false
@@ -1637,8 +1649,10 @@ paths:
           anyOf:
           - type: integer
           - type: 'null'
+          description: A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
           default: 20
           title: Limit
+        description: A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
       - name: order
         in: query
         required: false
@@ -1646,24 +1660,10 @@ paths:
           anyOf:
           - $ref: '#/components/schemas/Order'
           - type: 'null'
+          description: The order to return the input items in.
           default: desc
           title: Order
-      - name: response_id
-        in: path
-        required: true
-        schema:
-          type: string
-        description: 'Path parameter: response_id'
-      - name: include
-        in: query
-        required: false
-        schema:
-          anyOf:
-          - type: array
-            items:
-              type: string
-          - type: 'null'
-          title: Include
+        description: The order to return the input items in.
   /v1/safety/run-shield:
     post:
       responses:
@@ -5928,15 +5928,16 @@ components:
       title: OpenAIResponseUsage
       description: Usage information for OpenAI response.
     ResponseGuardrailSpec:
-      description: Specification for a guardrail to apply during response generation.
       properties:
         type:
-          title: Type
           type: string
+          title: Type
+      additionalProperties: false
+      type: object
       required:
       - type
       title: ResponseGuardrailSpec
-      type: object
+      description: Specification for a guardrail to apply during response generation.
     OpenAIResponseInputTool:
       discriminator:
         mapping:
@@ -6010,178 +6011,6 @@ components:
       - server_label
       title: OpenAIResponseInputToolMCP
       description: Model Context Protocol (MCP) tool configuration for OpenAI response inputs.
-    CreateOpenaiResponseRequest:
-      properties:
-        input:
-          anyOf:
-          - type: string
-          - items:
-              anyOf:
-              - oneOf:
-                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                  title: OpenAIResponseMessage-Input
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                  title: OpenAIResponseOutputMessageWebSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                  title: OpenAIResponseOutputMessageFileSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                  title: OpenAIResponseOutputMessageFunctionToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                  title: OpenAIResponseOutputMessageMCPCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                  title: OpenAIResponseOutputMessageMCPListTools
-                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                  title: OpenAIResponseMCPApprovalRequest
-                discriminator:
-                  propertyName: type
-                  mapping:
-                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                    message: '#/components/schemas/OpenAIResponseMessage-Input'
-                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                title: OpenAIResponseMessage-Input | ... (7 variants)
-              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
-                title: OpenAIResponseInputFunctionToolCallOutput
-              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
-                title: OpenAIResponseMCPApprovalResponse
-              - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                title: OpenAIResponseMessage-Input
-              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse | OpenAIResponseMessage-Input
-            type: array
-            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-        model:
-          type: string
-          title: Model
-        prompt:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponsePrompt'
-            title: OpenAIResponsePrompt
-          - type: 'null'
-          title: OpenAIResponsePrompt
-        instructions:
-          anyOf:
-          - type: string
-          - type: 'null'
-        parallel_tool_calls:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        previous_response_id:
-          anyOf:
-          - type: string
-          - type: 'null'
-        conversation:
-          anyOf:
-          - type: string
-          - type: 'null'
-        store:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        stream:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: false
-        temperature:
-          anyOf:
-          - type: number
-          - type: 'null'
-        text:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseText'
-            title: OpenAIResponseText
-          - type: 'null'
-          title: OpenAIResponseText
-        tool_choice:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
-            title: OpenAIResponseInputToolChoiceMode
-          - oneOf:
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-              title: OpenAIResponseInputToolChoiceAllowedTools
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-              title: OpenAIResponseInputToolChoiceFileSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-              title: OpenAIResponseInputToolChoiceWebSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-              title: OpenAIResponseInputToolChoiceFunctionTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-              title: OpenAIResponseInputToolChoiceMCPTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-              title: OpenAIResponseInputToolChoiceCustomTool
-            discriminator:
-              propertyName: type
-              mapping:
-                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
-          - type: 'null'
-          title: OpenAIResponseInputToolChoiceMode
-        tools:
-          anyOf:
-          - items:
-              oneOf:
-              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                title: OpenAIResponseInputToolWebSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                title: OpenAIResponseInputToolFileSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
-                title: OpenAIResponseInputToolFunction
-              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
-                title: OpenAIResponseInputToolMCP
-              discriminator:
-                propertyName: type
-                mapping:
-                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
-                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
-                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
-            type: array
-          - type: 'null'
-        include:
-          anyOf:
-          - items:
-              $ref: '#/components/schemas/ResponseItemInclude'
-            type: array
-          - type: 'null'
-        max_infer_iters:
-          anyOf:
-          - type: integer
-          - type: 'null'
-          default: 10
-        max_tool_calls:
-          anyOf:
-          - type: integer
-          - type: 'null'
-        metadata:
-          anyOf:
-          - additionalProperties:
-              type: string
-            type: object
-          - type: 'null'
-      type: object
-      required:
-      - input
-      - model
-      title: CreateOpenaiResponseRequest
     OpenAIResponseObject:
       properties:
         created_at:
@@ -10600,6 +10429,211 @@ components:
       - reasoning.encrypted_content
       title: ConversationItemInclude
       description: Specify additional output data to include in the model response.
+    CreateResponseRequest:
+      properties:
+        input:
+          anyOf:
+          - type: string
+          - items:
+              anyOf:
+              - oneOf:
+                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
+                  title: OpenAIResponseMessage-Input
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                  title: OpenAIResponseOutputMessageWebSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                  title: OpenAIResponseOutputMessageFileSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                  title: OpenAIResponseOutputMessageFunctionToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                  title: OpenAIResponseOutputMessageMCPCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                  title: OpenAIResponseOutputMessageMCPListTools
+                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                  title: OpenAIResponseMCPApprovalRequest
+                discriminator:
+                  propertyName: type
+                  mapping:
+                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                    message: '#/components/schemas/OpenAIResponseMessage-Input'
+                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                title: OpenAIResponseMessage-Input | ... (7 variants)
+              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
+                title: OpenAIResponseInputFunctionToolCallOutput
+              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
+                title: OpenAIResponseMCPApprovalResponse
+              - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
+                title: OpenAIResponseMessage-Input
+              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse | OpenAIResponseMessage-Input
+            type: array
+            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
+          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
+          description: Input message(s) to create the response.
+        model:
+          type: string
+          title: Model
+          description: The underlying LLM used for completions.
+        prompt:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponsePrompt'
+            title: OpenAIResponsePrompt
+          - type: 'null'
+          description: Prompt object with ID, version, and variables.
+          title: OpenAIResponsePrompt
+        instructions:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Instructions to guide the model's behavior.
+        parallel_tool_calls:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to enable parallel tool calls.
+          default: true
+        previous_response_id:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a previous response to continue from.
+        conversation:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a conversation to add the response to.
+        store:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to store the response in the database.
+          default: true
+        stream:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to stream the response.
+          default: false
+        temperature:
+          anyOf:
+          - type: number
+            maximum: 2.0
+            minimum: 0.0
+          - type: 'null'
+          description: Sampling temperature.
+        text:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseText'
+            title: OpenAIResponseText
+          - type: 'null'
+          description: Configuration for text response generation.
+          title: OpenAIResponseText
+        tool_choice:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
+            title: OpenAIResponseInputToolChoiceMode
+          - oneOf:
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+              title: OpenAIResponseInputToolChoiceAllowedTools
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+              title: OpenAIResponseInputToolChoiceFileSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+              title: OpenAIResponseInputToolChoiceWebSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+              title: OpenAIResponseInputToolChoiceFunctionTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+              title: OpenAIResponseInputToolChoiceMCPTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+              title: OpenAIResponseInputToolChoiceCustomTool
+            discriminator:
+              propertyName: type
+              mapping:
+                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
+          - type: 'null'
+          title: OpenAIResponseInputToolChoiceMode
+          description: How the model should select which tool to call (if any).
+        tools:
+          anyOf:
+          - items:
+              oneOf:
+              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                title: OpenAIResponseInputToolWebSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                title: OpenAIResponseInputToolFileSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
+                title: OpenAIResponseInputToolFunction
+              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
+                title: OpenAIResponseInputToolMCP
+              discriminator:
+                propertyName: type
+                mapping:
+                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
+                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
+                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
+            type: array
+          - type: 'null'
+          description: List of tools available to the model.
+        include:
+          anyOf:
+          - items:
+              $ref: '#/components/schemas/ResponseItemInclude'
+            type: array
+          - type: 'null'
+          description: Additional fields to include in the response.
+        max_infer_iters:
+          anyOf:
+          - type: integer
+            minimum: 1.0
+          - type: 'null'
+          description: Maximum number of inference iterations.
+          default: 10
+        guardrails:
+          anyOf:
+          - items:
+              anyOf:
+              - type: string
+              - $ref: '#/components/schemas/ResponseGuardrailSpec'
+                title: ResponseGuardrailSpec
+              title: string | ResponseGuardrailSpec
+            type: array
+          - type: 'null'
+          description: List of guardrails to apply during response generation.
+        max_tool_calls:
+          anyOf:
+          - type: integer
+          - type: 'null'
+          description: Max number of total calls to built-in tools that can be processed in a response.
+        metadata:
+          anyOf:
+          - additionalProperties:
+              type: string
+            type: object
+          - type: 'null'
+          description: Dictionary of metadata key-value pairs to attach to the response.
+      additionalProperties: false
+      type: object
+      required:
+      - input
+      - model
+      title: CreateResponseRequest
+      description: Request model for creating a response.
     DatasetPurpose:
       type: string
       enum:
@@ -12002,6 +12036,7 @@ components:
               - `get_adapter_impl(config, deps)`: returns the adapter implementation
 
               Example: `module: ramalama_stack`
+
           nullable: true
         pip_packages:
           description: The pip dependencies needed for this implementation
@@ -12075,6 +12110,7 @@ components:
               - `get_adapter_impl(config, deps)`: returns the adapter implementation
 
               Example: `module: ramalama_stack`
+
           nullable: true
         pip_packages:
           description: The pip dependencies needed for this implementation
@@ -12165,6 +12201,7 @@ components:
               - `get_adapter_impl(config, deps)`: returns the adapter implementation
 
               Example: `module: ramalama_stack`
+
           nullable: true
         pip_packages:
           description: The pip dependencies needed for this implementation
diff --git a/docs/static/stainless-llama-stack-spec.yaml b/docs/static/stainless-llama-stack-spec.yaml
index e86bf6bc11..68c12c7ae2 100644
--- a/docs/static/stainless-llama-stack-spec.yaml
+++ b/docs/static/stainless-llama-stack-spec.yaml
@@ -1472,7 +1472,7 @@ paths:
     get:
       responses:
         '200':
-          description: A ListOpenAIResponseObject.
+          description: Successful Response
           content:
             application/json:
               schema:
@@ -1491,7 +1491,7 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: List Openai Responses
+      summary: List all responses.
       description: List all responses.
       operationId: list_openai_responses_v1_responses_get
       parameters:
@@ -1502,7 +1502,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: The ID of the last response to return.
           title: After
+        description: The ID of the last response to return.
       - name: limit
         in: query
         required: false
@@ -1510,8 +1512,10 @@ paths:
           anyOf:
           - type: integer
           - type: 'null'
+          description: The number of responses to return.
           default: 50
           title: Limit
+        description: The number of responses to return.
       - name: model
         in: query
         required: false
@@ -1519,7 +1523,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: The model to filter responses by.
           title: Model
+        description: The model to filter responses by.
       - name: order
         in: query
         required: false
@@ -1527,12 +1533,14 @@ paths:
           anyOf:
           - $ref: '#/components/schemas/Order'
           - type: 'null'
+          description: The order to sort responses by when sorted by created_at ('asc' or 'desc').
           default: desc
           title: Order
+        description: The order to sort responses by when sorted by created_at ('asc' or 'desc').
     post:
       responses:
         '200':
-          description: An OpenAIResponseObject.
+          description: An OpenAIResponseObject or a stream of OpenAIResponseObjectStream.
           content:
             application/json:
               schema:
@@ -1554,7 +1562,7 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: Create Openai Response
+      summary: Create a model response.
       description: Create a model response.
       operationId: create_openai_response_v1_responses_post
       requestBody:
@@ -1562,55 +1570,31 @@ paths:
         content:
           application/json:
             schema:
-              $ref: '#/components/schemas/CreateOpenaiResponseRequest'
-        x-llama-stack-extra-body-params:
-          guardrails:
-            $defs:
-              ResponseGuardrailSpec:
-                description: |-
-                  Specification for a guardrail to apply during response generation.
-
-                  :param type: The type/identifier of the guardrail.
-                properties:
-                  type:
-                    title: Type
-                    type: string
-                required:
-                - type
-                title: ResponseGuardrailSpec
-                type: object
-            anyOf:
-            - items:
-                anyOf:
-                - type: string
-                - $ref: '#/components/schemas/ResponseGuardrailSpec'
-              type: array
-            - type: 'null'
-            description: List of guardrails to apply during response generation. Guardrails provide safety and content moderation.
+              $ref: '#/components/schemas/CreateResponseRequest'
   /v1/responses/{response_id}:
     get:
       responses:
         '200':
-          description: An OpenAIResponseObject.
+          description: Successful Response
           content:
             application/json:
               schema:
                 $ref: '#/components/schemas/OpenAIResponseObject'
         '400':
-          description: Bad Request
           $ref: '#/components/responses/BadRequest400'
+          description: Bad Request
         '429':
-          description: Too Many Requests
           $ref: '#/components/responses/TooManyRequests429'
+          description: Too Many Requests
         '500':
-          description: Internal Server Error
           $ref: '#/components/responses/InternalServerError500'
+          description: Internal Server Error
         default:
-          description: Default Response
           $ref: '#/components/responses/DefaultError'
+          description: Default Response
       tags:
       - Agents
-      summary: Get Openai Response
+      summary: Get a model response.
       description: Get a model response.
       operationId: get_openai_response_v1_responses__response_id__get
       parameters:
@@ -1619,30 +1603,32 @@ paths:
         required: true
         schema:
           type: string
-        description: 'Path parameter: response_id'
+          description: The ID of the OpenAI response to retrieve.
+          title: Response Id
+        description: The ID of the OpenAI response to retrieve.
     delete:
       responses:
         '200':
-          description: An OpenAIDeleteResponseObject
+          description: Successful Response
           content:
             application/json:
               schema:
                 $ref: '#/components/schemas/OpenAIDeleteResponseObject'
         '400':
-          description: Bad Request
           $ref: '#/components/responses/BadRequest400'
+          description: Bad Request
         '429':
-          description: Too Many Requests
           $ref: '#/components/responses/TooManyRequests429'
+          description: Too Many Requests
         '500':
-          description: Internal Server Error
           $ref: '#/components/responses/InternalServerError500'
+          description: Internal Server Error
         default:
-          description: Default Response
           $ref: '#/components/responses/DefaultError'
+          description: Default Response
       tags:
       - Agents
-      summary: Delete Openai Response
+      summary: Delete a response.
       description: Delete a response.
       operationId: delete_openai_response_v1_responses__response_id__delete
       parameters:
@@ -1651,12 +1637,14 @@ paths:
         required: true
         schema:
           type: string
-        description: 'Path parameter: response_id'
+          description: The ID of the OpenAI response to delete.
+          title: Response Id
+        description: The ID of the OpenAI response to delete.
   /v1/responses/{response_id}/input_items:
     get:
       responses:
         '200':
-          description: An ListOpenAIResponseInputItem.
+          description: Successful Response
           content:
             application/json:
               schema:
@@ -1675,10 +1663,18 @@ paths:
           description: Default Response
       tags:
       - Agents
-      summary: List Openai Response Input Items
+      summary: List input items.
       description: List input items.
       operationId: list_openai_response_input_items_v1_responses__response_id__input_items_get
       parameters:
+      - name: response_id
+        in: path
+        required: true
+        schema:
+          type: string
+          description: The ID of the response to retrieve input items for.
+          title: Response Id
+        description: The ID of the response to retrieve input items for.
       - name: after
         in: query
         required: false
@@ -1686,7 +1682,9 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: An item ID to list items after, used for pagination.
           title: After
+        description: An item ID to list items after, used for pagination.
       - name: before
         in: query
         required: false
@@ -1694,7 +1692,21 @@ paths:
           anyOf:
           - type: string
           - type: 'null'
+          description: An item ID to list items before, used for pagination.
           title: Before
+        description: An item ID to list items before, used for pagination.
+      - name: include
+        in: query
+        required: false
+        schema:
+          anyOf:
+          - type: array
+            items:
+              $ref: '#/components/schemas/ResponseItemInclude'
+          - type: 'null'
+          description: Additional fields to include in the response.
+          title: Include
+        description: Additional fields to include in the response.
       - name: limit
         in: query
         required: false
@@ -1702,8 +1714,10 @@ paths:
           anyOf:
           - type: integer
           - type: 'null'
+          description: A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
           default: 20
           title: Limit
+        description: A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
       - name: order
         in: query
         required: false
@@ -1711,24 +1725,10 @@ paths:
           anyOf:
           - $ref: '#/components/schemas/Order'
           - type: 'null'
+          description: The order to return the input items in.
           default: desc
           title: Order
-      - name: response_id
-        in: path
-        required: true
-        schema:
-          type: string
-        description: 'Path parameter: response_id'
-      - name: include
-        in: query
-        required: false
-        schema:
-          anyOf:
-          - type: array
-            items:
-              type: string
-          - type: 'null'
-          title: Include
+        description: The order to return the input items in.
   /v1/safety/run-shield:
     post:
       responses:
@@ -7462,15 +7462,16 @@ components:
       title: OpenAIResponseUsage
       description: Usage information for OpenAI response.
     ResponseGuardrailSpec:
-      description: Specification for a guardrail to apply during response generation.
       properties:
         type:
-          title: Type
           type: string
+          title: Type
+      additionalProperties: false
+      type: object
       required:
       - type
       title: ResponseGuardrailSpec
-      type: object
+      description: Specification for a guardrail to apply during response generation.
     OpenAIResponseInputTool:
       discriminator:
         mapping:
@@ -7544,178 +7545,6 @@ components:
       - server_label
       title: OpenAIResponseInputToolMCP
       description: Model Context Protocol (MCP) tool configuration for OpenAI response inputs.
-    CreateOpenaiResponseRequest:
-      properties:
-        input:
-          anyOf:
-          - type: string
-          - items:
-              anyOf:
-              - oneOf:
-                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                  title: OpenAIResponseMessage-Input
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                  title: OpenAIResponseOutputMessageWebSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                  title: OpenAIResponseOutputMessageFileSearchToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                  title: OpenAIResponseOutputMessageFunctionToolCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                  title: OpenAIResponseOutputMessageMCPCall
-                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                  title: OpenAIResponseOutputMessageMCPListTools
-                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                  title: OpenAIResponseMCPApprovalRequest
-                discriminator:
-                  propertyName: type
-                  mapping:
-                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
-                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
-                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
-                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
-                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
-                    message: '#/components/schemas/OpenAIResponseMessage-Input'
-                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
-                title: OpenAIResponseMessage-Input | ... (7 variants)
-              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
-                title: OpenAIResponseInputFunctionToolCallOutput
-              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
-                title: OpenAIResponseMCPApprovalResponse
-              - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
-                title: OpenAIResponseMessage-Input
-              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse | OpenAIResponseMessage-Input
-            type: array
-            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | ...]
-        model:
-          type: string
-          title: Model
-        prompt:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponsePrompt'
-            title: OpenAIResponsePrompt
-          - type: 'null'
-          title: OpenAIResponsePrompt
-        instructions:
-          anyOf:
-          - type: string
-          - type: 'null'
-        parallel_tool_calls:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        previous_response_id:
-          anyOf:
-          - type: string
-          - type: 'null'
-        conversation:
-          anyOf:
-          - type: string
-          - type: 'null'
-        store:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: true
-        stream:
-          anyOf:
-          - type: boolean
-          - type: 'null'
-          default: false
-        temperature:
-          anyOf:
-          - type: number
-          - type: 'null'
-        text:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseText'
-            title: OpenAIResponseText
-          - type: 'null'
-          title: OpenAIResponseText
-        tool_choice:
-          anyOf:
-          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
-            title: OpenAIResponseInputToolChoiceMode
-          - oneOf:
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-              title: OpenAIResponseInputToolChoiceAllowedTools
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-              title: OpenAIResponseInputToolChoiceFileSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-              title: OpenAIResponseInputToolChoiceWebSearch
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-              title: OpenAIResponseInputToolChoiceFunctionTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-              title: OpenAIResponseInputToolChoiceMCPTool
-            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-              title: OpenAIResponseInputToolChoiceCustomTool
-            discriminator:
-              propertyName: type
-              mapping:
-                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
-                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
-                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
-                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
-                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
-                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
-            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
-          - type: 'null'
-          title: OpenAIResponseInputToolChoiceMode
-        tools:
-          anyOf:
-          - items:
-              oneOf:
-              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                title: OpenAIResponseInputToolWebSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                title: OpenAIResponseInputToolFileSearch
-              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
-                title: OpenAIResponseInputToolFunction
-              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
-                title: OpenAIResponseInputToolMCP
-              discriminator:
-                propertyName: type
-                mapping:
-                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
-                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
-                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
-                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
-              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
-            type: array
-          - type: 'null'
-        include:
-          anyOf:
-          - items:
-              $ref: '#/components/schemas/ResponseItemInclude'
-            type: array
-          - type: 'null'
-        max_infer_iters:
-          anyOf:
-          - type: integer
-          - type: 'null'
-          default: 10
-        max_tool_calls:
-          anyOf:
-          - type: integer
-          - type: 'null'
-        metadata:
-          anyOf:
-          - additionalProperties:
-              type: string
-            type: object
-          - type: 'null'
-      type: object
-      required:
-      - input
-      - model
-      title: CreateOpenaiResponseRequest
     OpenAIResponseObject:
       properties:
         created_at:
@@ -12210,6 +12039,209 @@ components:
       - reasoning.encrypted_content
       title: ConversationItemInclude
       description: Specify additional output data to include in the model response.
+    CreateResponseRequest:
+      properties:
+        input:
+          anyOf:
+          - type: string
+          - items:
+              anyOf:
+              - oneOf:
+                - $ref: '#/components/schemas/OpenAIResponseMessage-Input'
+                  title: OpenAIResponseMessage-Input
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                  title: OpenAIResponseOutputMessageWebSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                  title: OpenAIResponseOutputMessageFileSearchToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                  title: OpenAIResponseOutputMessageFunctionToolCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                  title: OpenAIResponseOutputMessageMCPCall
+                - $ref: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                  title: OpenAIResponseOutputMessageMCPListTools
+                - $ref: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                  title: OpenAIResponseMCPApprovalRequest
+                discriminator:
+                  propertyName: type
+                  mapping:
+                    file_search_call: '#/components/schemas/OpenAIResponseOutputMessageFileSearchToolCall'
+                    function_call: '#/components/schemas/OpenAIResponseOutputMessageFunctionToolCall'
+                    mcp_approval_request: '#/components/schemas/OpenAIResponseMCPApprovalRequest'
+                    mcp_call: '#/components/schemas/OpenAIResponseOutputMessageMCPCall'
+                    mcp_list_tools: '#/components/schemas/OpenAIResponseOutputMessageMCPListTools'
+                    message: '#/components/schemas/OpenAIResponseMessage-Input'
+                    web_search_call: '#/components/schemas/OpenAIResponseOutputMessageWebSearchToolCall'
+                title: OpenAIResponseMessage-Input | ... (7 variants)
+              - $ref: '#/components/schemas/OpenAIResponseInputFunctionToolCallOutput'
+                title: OpenAIResponseInputFunctionToolCallOutput
+              - $ref: '#/components/schemas/OpenAIResponseMCPApprovalResponse'
+                title: OpenAIResponseMCPApprovalResponse
+              title: OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse
+            type: array
+            title: list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse]
+          title: string | list[OpenAIResponseMessageUnion | OpenAIResponseInputFunctionToolCallOutput | OpenAIResponseMCPApprovalResponse]
+          description: Input message(s) to create the response.
+        model:
+          type: string
+          title: Model
+          description: The underlying LLM used for completions.
+        prompt:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponsePrompt'
+            title: OpenAIResponsePrompt
+          - type: 'null'
+          description: Prompt object with ID, version, and variables.
+          title: OpenAIResponsePrompt
+        instructions:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Instructions to guide the model's behavior.
+        parallel_tool_calls:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to enable parallel tool calls.
+          default: true
+        previous_response_id:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a previous response to continue from.
+        conversation:
+          anyOf:
+          - type: string
+          - type: 'null'
+          description: Optional ID of a conversation to add the response to.
+        store:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to store the response in the database.
+          default: true
+        stream:
+          anyOf:
+          - type: boolean
+          - type: 'null'
+          description: Whether to stream the response.
+          default: false
+        temperature:
+          anyOf:
+          - type: number
+            maximum: 2.0
+            minimum: 0.0
+          - type: 'null'
+          description: Sampling temperature.
+        text:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseText'
+            title: OpenAIResponseText
+          - type: 'null'
+          description: Configuration for text response generation.
+          title: OpenAIResponseText
+        tool_choice:
+          anyOf:
+          - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMode'
+            title: OpenAIResponseInputToolChoiceMode
+          - oneOf:
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+              title: OpenAIResponseInputToolChoiceAllowedTools
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+              title: OpenAIResponseInputToolChoiceFileSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+              title: OpenAIResponseInputToolChoiceWebSearch
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+              title: OpenAIResponseInputToolChoiceFunctionTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+              title: OpenAIResponseInputToolChoiceMCPTool
+            - $ref: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+              title: OpenAIResponseInputToolChoiceCustomTool
+            discriminator:
+              propertyName: type
+              mapping:
+                allowed_tools: '#/components/schemas/OpenAIResponseInputToolChoiceAllowedTools'
+                custom: '#/components/schemas/OpenAIResponseInputToolChoiceCustomTool'
+                file_search: '#/components/schemas/OpenAIResponseInputToolChoiceFileSearch'
+                function: '#/components/schemas/OpenAIResponseInputToolChoiceFunctionTool'
+                mcp: '#/components/schemas/OpenAIResponseInputToolChoiceMCPTool'
+                web_search: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+                web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolChoiceWebSearch'
+            title: OpenAIResponseInputToolChoiceAllowedTools | ... (6 variants)
+          - type: 'null'
+          title: OpenAIResponseInputToolChoiceMode
+          description: How the model should select which tool to call (if any).
+        tools:
+          anyOf:
+          - items:
+              oneOf:
+              - $ref: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                title: OpenAIResponseInputToolWebSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                title: OpenAIResponseInputToolFileSearch
+              - $ref: '#/components/schemas/OpenAIResponseInputToolFunction'
+                title: OpenAIResponseInputToolFunction
+              - $ref: '#/components/schemas/OpenAIResponseInputToolMCP'
+                title: OpenAIResponseInputToolMCP
+              discriminator:
+                propertyName: type
+                mapping:
+                  file_search: '#/components/schemas/OpenAIResponseInputToolFileSearch'
+                  function: '#/components/schemas/OpenAIResponseInputToolFunction'
+                  mcp: '#/components/schemas/OpenAIResponseInputToolMCP'
+                  web_search: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_2025_08_26: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+                  web_search_preview_2025_03_11: '#/components/schemas/OpenAIResponseInputToolWebSearch'
+              title: OpenAIResponseInputToolWebSearch | ... (4 variants)
+            type: array
+          - type: 'null'
+          description: List of tools available to the model.
+        include:
+          anyOf:
+          - items:
+              $ref: '#/components/schemas/ResponseItemInclude'
+            type: array
+          - type: 'null'
+          description: Additional fields to include in the response.
+        max_infer_iters:
+          anyOf:
+          - type: integer
+            minimum: 1.0
+          - type: 'null'
+          description: Maximum number of inference iterations.
+          default: 10
+        guardrails:
+          anyOf:
+          - items:
+              anyOf:
+              - type: string
+              - $ref: '#/components/schemas/ResponseGuardrailSpec'
+                title: ResponseGuardrailSpec
+              title: string | ResponseGuardrailSpec
+            type: array
+          - type: 'null'
+          description: List of guardrails to apply during response generation.
+        max_tool_calls:
+          anyOf:
+          - type: integer
+          - type: 'null'
+          description: Max number of total calls to built-in tools that can be processed in a response.
+        metadata:
+          anyOf:
+          - additionalProperties:
+              type: string
+            type: object
+          - type: 'null'
+          description: Dictionary of metadata key-value pairs to attach to the response.
+      additionalProperties: false
+      type: object
+      required:
+      - input
+      - model
+      title: CreateResponseRequest
+      description: Request model for creating a response.
     DatasetPurpose:
       type: string
       enum:
diff --git a/scripts/integration-tests.sh b/scripts/integration-tests.sh
index 86385b9820..194a27ad80 100755
--- a/scripts/integration-tests.sh
+++ b/scripts/integration-tests.sh
@@ -435,6 +435,7 @@ if [[ "$STACK_CONFIG" == *"docker:"* && "$COLLECT_ONLY" == false ]]; then
     DOCKER_ENV_VARS="$DOCKER_ENV_VARS -e LLAMA_STACK_TEST_INFERENCE_MODE=$INFERENCE_MODE"
     DOCKER_ENV_VARS="$DOCKER_ENV_VARS -e LLAMA_STACK_TEST_STACK_CONFIG_TYPE=server"
     DOCKER_ENV_VARS="$DOCKER_ENV_VARS -e LLAMA_STACK_TEST_MCP_HOST=${LLAMA_STACK_TEST_MCP_HOST:-host.docker.internal}"
+    DOCKER_ENV_VARS="$DOCKER_ENV_VARS -e OTEL_SDK_DISABLED=true"
     # Disabled: https://github.com/llamastack/llama-stack/issues/4089
     #DOCKER_ENV_VARS="$DOCKER_ENV_VARS -e OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:${COLLECTOR_PORT}"
     DOCKER_ENV_VARS="$DOCKER_ENV_VARS -e OTEL_METRIC_EXPORT_INTERVAL=200"
diff --git a/scripts/openapi_generator/main.py b/scripts/openapi_generator/main.py
index e881ff7262..e7aeba7f97 100755
--- a/scripts/openapi_generator/main.py
+++ b/scripts/openapi_generator/main.py
@@ -92,6 +92,7 @@ def generate_openapi_spec(output_dir: str) -> dict[str, Any]:
 
     # Apply duplicate union extraction to combined schema (used by Stainless)
     combined_schema = schema_transforms._extract_duplicate_union_types(combined_schema)
+    combined_schema = schema_transforms._dedupe_create_response_request_input_union_for_stainless(combined_schema)
 
     base_description = (
         "This is the specification of the Llama Stack that provides\n"
diff --git a/scripts/openapi_generator/schema_transforms.py b/scripts/openapi_generator/schema_transforms.py
index 5821c99d58..17c65c4d06 100644
--- a/scripts/openapi_generator/schema_transforms.py
+++ b/scripts/openapi_generator/schema_transforms.py
@@ -817,6 +817,90 @@ def _extract_duplicate_union_types(openapi_schema: dict[str, Any]) -> dict[str,
     return openapi_schema
 
 
+def _dedupe_create_response_request_input_union_for_stainless(openapi_schema: dict[str, Any]) -> dict[str, Any]:
+    """Deduplicate inline unions in `CreateResponseRequest.input` for Stainless.
+
+    The stable OpenAPI spec intentionally preserves legacy structure for oasdiff
+    compatibility, but Stainless codegen treats duplicated inline unions as separate
+    types and can generate name clashes.
+
+    This transform is intended to run only on the combined (stainless) spec.
+    """
+    if "components" not in openapi_schema or "schemas" not in openapi_schema["components"]:
+        return openapi_schema
+
+    schemas = openapi_schema["components"]["schemas"]
+    create_request = schemas.get("CreateResponseRequest")
+    if not isinstance(create_request, dict):
+        return openapi_schema
+
+    properties = create_request.get("properties")
+    if not isinstance(properties, dict):
+        return openapi_schema
+
+    input_prop = properties.get("input")
+    if not isinstance(input_prop, dict):
+        return openapi_schema
+
+    any_of = input_prop.get("anyOf")
+    if not isinstance(any_of, list):
+        return openapi_schema
+
+    array_schema: dict[str, Any] | None = None
+    for item in any_of:
+        if isinstance(item, dict) and (item.get("type") == "array" or "items" in item):
+            array_schema = item
+            break
+
+    if array_schema is None:
+        return openapi_schema
+
+    items_schema = array_schema.get("items")
+    if not isinstance(items_schema, dict):
+        return openapi_schema
+
+    items_any_of = items_schema.get("anyOf")
+    if not isinstance(items_any_of, list):
+        return openapi_schema
+
+    def _collect_refs(obj: Any, refs: set[str]) -> None:
+        if isinstance(obj, dict):
+            ref = obj.get("$ref")
+            if isinstance(ref, str):
+                refs.add(ref)
+            for value in obj.values():
+                _collect_refs(value, refs)
+        elif isinstance(obj, list):
+            for value in obj:
+                _collect_refs(value, refs)
+
+    def _is_direct_ref_item(item: dict[str, Any]) -> bool:
+        if "$ref" not in item:
+            return False
+        # Direct refs are the sibling entries like: {$ref: ..., title: ...}
+        # Union/nesting containers also include oneOf/anyOf/items/etc.
+        container_keys = {"oneOf", "anyOf", "items", "properties", "additionalProperties"}
+        return not any(key in item for key in container_keys)
+
+    refs_in_nested_unions: set[str] = set()
+    for item in items_any_of:
+        if isinstance(item, dict) and not _is_direct_ref_item(item):
+            _collect_refs(item, refs_in_nested_unions)
+
+    if not refs_in_nested_unions:
+        return openapi_schema
+
+    # Remove sibling direct refs that are duplicates of refs found elsewhere
+    deduplicated: list[Any] = []
+    for item in items_any_of:
+        if isinstance(item, dict) and _is_direct_ref_item(item) and item["$ref"] in refs_in_nested_unions:
+            continue
+        deduplicated.append(item)
+
+    items_schema["anyOf"] = deduplicated
+    return openapi_schema
+
+
 def _convert_multiline_strings_to_literal(obj: Any) -> Any:
     """Recursively convert multi-line strings to LiteralScalarString for YAML block scalar formatting."""
     try:
diff --git a/src/llama_stack/core/library_client.py b/src/llama_stack/core/library_client.py
index 9d2ed3953b..289da874c7 100644
--- a/src/llama_stack/core/library_client.py
+++ b/src/llama_stack/core/library_client.py
@@ -428,10 +428,26 @@ async def _call_streaming(
         body = self._convert_body(func, body)
 
         async def gen():
-            async for chunk in await func(**body):
-                data = json.dumps(convert_pydantic_to_json_value(chunk))
-                sse_event = f"data: {data}\n\n"
-                yield sse_event.encode("utf-8")
+            result = await func(**body)
+
+            # Handle FastAPI StreamingResponse (returned by router endpoints)
+            # Extract the async generator from the StreamingResponse body
+            from fastapi.responses import StreamingResponse
+
+            if isinstance(result, StreamingResponse):
+                # StreamingResponse.body_iterator is the async generator
+                async for chunk in result.body_iterator:
+                    # Chunk is already SSE-formatted string from sse_generator, encode to bytes
+                    if isinstance(chunk, str):
+                        yield chunk.encode("utf-8")
+                    else:
+                        yield chunk
+            else:
+                # Direct async generator from implementation
+                async for chunk in result:
+                    data = json.dumps(convert_pydantic_to_json_value(chunk))
+                    sse_event = f"data: {data}\n\n"
+                    yield sse_event.encode("utf-8")
 
         wrapped_gen = preserve_contexts_async_generator(gen(), [PROVIDER_DATA_VAR])
 
diff --git a/src/llama_stack/core/server/fastapi_router_registry.py b/src/llama_stack/core/server/fastapi_router_registry.py
index 71e5e67a33..b5535c51d3 100644
--- a/src/llama_stack/core/server/fastapi_router_registry.py
+++ b/src/llama_stack/core/server/fastapi_router_registry.py
@@ -18,6 +18,7 @@
 
 from llama_stack_api import (
     admin,
+    agents,
     batches,
     benchmarks,
     conversations,
@@ -41,6 +42,7 @@
 # Add new APIs here as they are migrated to the router system
 _ROUTER_FACTORIES: dict[str, Callable[[Any], APIRouter]] = {
     "admin": admin.fastapi_routes.create_router,
+    "agents": agents.fastapi_routes.create_router,
     "batches": batches.fastapi_routes.create_router,
     "benchmarks": benchmarks.fastapi_routes.create_router,
     "conversations": conversations.fastapi_routes.create_router,
diff --git a/src/llama_stack/providers/inline/agents/meta_reference/agents.py b/src/llama_stack/providers/inline/agents/meta_reference/agents.py
index cc085135f8..693bc0bf3b 100644
--- a/src/llama_stack/providers/inline/agents/meta_reference/agents.py
+++ b/src/llama_stack/providers/inline/agents/meta_reference/agents.py
@@ -4,6 +4,7 @@
 # This source code is licensed under the terms described in the LICENSE file in
 # the root directory of this source tree.
 
+from collections.abc import AsyncIterator
 
 from llama_stack.core.datatypes import AccessRule
 from llama_stack.core.storage.kvstore import InmemoryKVStoreImpl, kvstore_impl
@@ -13,20 +14,19 @@
     Agents,
     Connectors,
     Conversations,
+    CreateResponseRequest,
+    DeleteResponseRequest,
     Files,
     Inference,
     ListOpenAIResponseInputItem,
     ListOpenAIResponseObject,
+    ListResponseInputItemsRequest,
+    ListResponsesRequest,
     OpenAIDeleteResponseObject,
-    OpenAIResponseInput,
-    OpenAIResponseInputTool,
-    OpenAIResponseInputToolChoice,
     OpenAIResponseObject,
-    OpenAIResponsePrompt,
-    OpenAIResponseText,
-    Order,
+    OpenAIResponseObjectStream,
     Prompts,
-    ResponseGuardrail,
+    RetrieveResponseRequest,
     Safety,
     ToolGroups,
     ToolRuntime,
@@ -92,79 +92,69 @@ async def shutdown(self) -> None:
     # OpenAI responses
     async def get_openai_response(
         self,
-        response_id: str,
+        request: RetrieveResponseRequest,
     ) -> OpenAIResponseObject:
         assert self.openai_responses_impl is not None, "OpenAI responses not initialized"
-        return await self.openai_responses_impl.get_openai_response(response_id)
+        return await self.openai_responses_impl.get_openai_response(request.response_id)
 
     async def create_openai_response(
         self,
-        input: str | list[OpenAIResponseInput],
-        model: str,
-        prompt: OpenAIResponsePrompt | None = None,
-        instructions: str | None = None,
-        parallel_tool_calls: bool | None = True,
-        previous_response_id: str | None = None,
-        conversation: str | None = None,
-        store: bool | None = True,
-        stream: bool | None = False,
-        temperature: float | None = None,
-        text: OpenAIResponseText | None = None,
-        tool_choice: OpenAIResponseInputToolChoice | None = None,
-        tools: list[OpenAIResponseInputTool] | None = None,
-        include: list[str] | None = None,
-        max_infer_iters: int | None = 10,
-        guardrails: list[ResponseGuardrail] | None = None,
-        max_tool_calls: int | None = None,
-        metadata: dict[str, str] | None = None,
-    ) -> OpenAIResponseObject:
+        request: CreateResponseRequest,
+    ) -> OpenAIResponseObject | AsyncIterator[OpenAIResponseObjectStream]:
+        """Create an OpenAI response.
+
+        Returns either a single response object (non-streaming) or an async iterator
+        yielding response stream events (streaming).
+        """
         assert self.openai_responses_impl is not None, "OpenAI responses not initialized"
         result = await self.openai_responses_impl.create_openai_response(
-            input,
-            model,
-            prompt,
-            instructions,
-            previous_response_id,
-            conversation,
-            store,
-            stream,
-            temperature,
-            text,
-            tool_choice,
-            tools,
-            include,
-            max_infer_iters,
-            guardrails,
-            parallel_tool_calls,
-            max_tool_calls,
-            metadata,
+            request.input,
+            request.model,
+            request.prompt,
+            request.instructions,
+            request.previous_response_id,
+            request.conversation,
+            request.store,
+            request.stream,
+            request.temperature,
+            request.text,
+            request.tool_choice,
+            request.tools,
+            request.include,
+            request.max_infer_iters,
+            request.guardrails,
+            request.parallel_tool_calls,
+            request.max_tool_calls,
+            request.metadata,
         )
-        return result  # type: ignore[no-any-return]
+        return result
 
     async def list_openai_responses(
         self,
-        after: str | None = None,
-        limit: int | None = 50,
-        model: str | None = None,
-        order: Order | None = Order.desc,
+        request: ListResponsesRequest,
     ) -> ListOpenAIResponseObject:
         assert self.openai_responses_impl is not None, "OpenAI responses not initialized"
-        return await self.openai_responses_impl.list_openai_responses(after, limit, model, order)
+        return await self.openai_responses_impl.list_openai_responses(
+            request.after, request.limit, request.model, request.order
+        )
 
     async def list_openai_response_input_items(
         self,
-        response_id: str,
-        after: str | None = None,
-        before: str | None = None,
-        include: list[str] | None = None,
-        limit: int | None = 20,
-        order: Order | None = Order.desc,
+        request: ListResponseInputItemsRequest,
     ) -> ListOpenAIResponseInputItem:
         assert self.openai_responses_impl is not None, "OpenAI responses not initialized"
         return await self.openai_responses_impl.list_openai_response_input_items(
-            response_id, after, before, include, limit, order
+            request.response_id,
+            request.after,
+            request.before,
+            request.include,
+            request.limit,
+            request.order,
         )
 
-    async def delete_openai_response(self, response_id: str) -> OpenAIDeleteResponseObject:
+    async def delete_openai_response(
+        self,
+        request: DeleteResponseRequest,
+    ) -> OpenAIDeleteResponseObject:
         assert self.openai_responses_impl is not None, "OpenAI responses not initialized"
-        return await self.openai_responses_impl.delete_openai_response(response_id)
+        return await self.openai_responses_impl.delete_openai_response(request.response_id)
diff --git a/src/llama_stack_api/__init__.py b/src/llama_stack_api/__init__.py
index ccadda611f..5571048a1d 100644
--- a/src/llama_stack_api/__init__.py
+++ b/src/llama_stack_api/__init__.py
@@ -37,7 +37,17 @@
 )
 
 # Import all public API symbols
-from .agents import Agents, ResponseGuardrail, ResponseGuardrailSpec, ResponseItemInclude
+from .agents import (
+    Agents,
+    CreateResponseRequest,
+    DeleteResponseRequest,
+    ListResponseInputItemsRequest,
+    ListResponsesRequest,
+    ResponseGuardrail,
+    ResponseGuardrailSpec,
+    ResponseItemInclude,
+    RetrieveResponseRequest,
+)
 from .batches import (
     Batches,
     BatchObject,
@@ -559,6 +569,12 @@
     # API Symbols
     "Agents",
     "AggregationFunctionType",
+    # Agents Request Models
+    "CreateResponseRequest",
+    "DeleteResponseRequest",
+    "ListResponseInputItemsRequest",
+    "ListResponsesRequest",
+    "RetrieveResponseRequest",
     "AlgorithmConfig",
     "AllowedToolsFilter",
     "Api",
diff --git a/src/llama_stack_api/agents.py b/src/llama_stack_api/agents.py
deleted file mode 100644
index 63e6f0fd1f..0000000000
--- a/src/llama_stack_api/agents.py
+++ /dev/null
@@ -1,173 +0,0 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-# All rights reserved.
-#
-# This source code is licensed under the terms described in the LICENSE file in
-# the root directory of this source tree.
-
-from collections.abc import AsyncIterator
-from enum import StrEnum
-from typing import Annotated, Protocol, runtime_checkable
-
-from pydantic import BaseModel
-
-from llama_stack_api.common.responses import Order
-from llama_stack_api.schema_utils import ExtraBodyField, json_schema_type, webmethod
-from llama_stack_api.version import LLAMA_STACK_API_V1
-
-from .openai_responses import (
-    ListOpenAIResponseInputItem,
-    ListOpenAIResponseObject,
-    OpenAIDeleteResponseObject,
-    OpenAIResponseInput,
-    OpenAIResponseInputTool,
-    OpenAIResponseInputToolChoice,
-    OpenAIResponseObject,
-    OpenAIResponseObjectStream,
-    OpenAIResponsePrompt,
-    OpenAIResponseText,
-)
-
-
-@json_schema_type
-class ResponseGuardrailSpec(BaseModel):
-    """Specification for a guardrail to apply during response generation.
-
-    :param type: The type/identifier of the guardrail.
-    """
-
-    type: str
-    # TODO: more fields to be added for guardrail configuration
-
-
-ResponseGuardrail = str | ResponseGuardrailSpec
-
-
-class ResponseItemInclude(StrEnum):
-    """
-    Specify additional output data to include in the model response.
-    """
-
-    web_search_call_action_sources = "web_search_call.action.sources"
-    code_interpreter_call_outputs = "code_interpreter_call.outputs"
-    computer_call_output_output_image_url = "computer_call_output.output.image_url"
-    file_search_call_results = "file_search_call.results"
-    message_input_image_image_url = "message.input_image.image_url"
-    message_output_text_logprobs = "message.output_text.logprobs"
-    reasoning_encrypted_content = "reasoning.encrypted_content"
-
-
-@runtime_checkable
-class Agents(Protocol):
-    """Agents
-
-    APIs for creating and interacting with agentic systems."""
-
-    # We situate the OpenAI Responses API in the Agents API just like we did things
-    # for Inference. The Responses API, in its intent, serves the same purpose as
-    # the Agents API above -- it is essentially a lightweight "agentic loop" with
-    # integrated tool calling.
-    #
-    # Both of these APIs are inherently stateful.
-
-    @webmethod(route="/responses/{response_id}", method="GET", level=LLAMA_STACK_API_V1)
-    async def get_openai_response(
-        self,
-        response_id: str,
-    ) -> OpenAIResponseObject:
-        """Get a model response.
-
-        :param response_id: The ID of the OpenAI response to retrieve.
-        :returns: An OpenAIResponseObject.
-        """
-        ...
-
-    @webmethod(route="/responses", method="POST", level=LLAMA_STACK_API_V1)
-    async def create_openai_response(
-        self,
-        input: str | list[OpenAIResponseInput],
-        model: str,
-        prompt: OpenAIResponsePrompt | None = None,
-        instructions: str | None = None,
-        parallel_tool_calls: bool | None = True,
-        previous_response_id: str | None = None,
-        conversation: str | None = None,
-        store: bool | None = True,
-        stream: bool | None = False,
-        temperature: float | None = None,
-        text: OpenAIResponseText | None = None,
-        tool_choice: OpenAIResponseInputToolChoice | None = None,
-        tools: list[OpenAIResponseInputTool] | None = None,
-        include: list[ResponseItemInclude] | None = None,
-        max_infer_iters: int | None = 10,  # this is an extension to the OpenAI API
-        guardrails: Annotated[
-            list[ResponseGuardrail] | None,
-            ExtraBodyField(
-                "List of guardrails to apply during response generation. Guardrails provide safety and content moderation."
-            ),
-        ] = None,
-        max_tool_calls: int | None = None,
-        metadata: dict[str, str] | None = None,
-    ) -> OpenAIResponseObject | AsyncIterator[OpenAIResponseObjectStream]:
-        """Create a model response.
-
-        :param input: Input message(s) to create the response.
-        :param model: The underlying LLM used for completions.
-        :param prompt: (Optional) Prompt object with ID, version, and variables.
-        :param previous_response_id: (Optional) if specified, the new response will be a continuation of the previous response. This can be used to easily fork-off new responses from existing responses.
-        :param conversation: (Optional) The ID of a conversation to add the response to. Must begin with 'conv_'. Input and output messages will be automatically added to the conversation.
-        :param include: (Optional) Additional fields to include in the response.
-        :param guardrails: (Optional) List of guardrails to apply during response generation. Can be guardrail IDs (strings) or guardrail specifications.
-        :param max_tool_calls: (Optional) Max number of total calls to built-in tools that can be processed in a response.
-        :param metadata: (Optional) Dictionary of metadata key-value pairs to attach to the response.
-        :returns: An OpenAIResponseObject.
-        """
-        ...
-
-    @webmethod(route="/responses", method="GET", level=LLAMA_STACK_API_V1)
-    async def list_openai_responses(
-        self,
-        after: str | None = None,
-        limit: int | None = 50,
-        model: str | None = None,
-        order: Order | None = Order.desc,
-    ) -> ListOpenAIResponseObject:
-        """List all responses.
-
-        :param after: The ID of the last response to return.
-        :param limit: The number of responses to return.
-        :param model: The model to filter responses by.
-        :param order: The order to sort responses by when sorted by created_at ('asc' or 'desc').
-        :returns: A ListOpenAIResponseObject.
-        """
-        ...
-
-    @webmethod(route="/responses/{response_id}/input_items", method="GET", level=LLAMA_STACK_API_V1)
-    async def list_openai_response_input_items(
-        self,
-        response_id: str,
-        after: str | None = None,
-        before: str | None = None,
-        include: list[str] | None = None,
-        limit: int | None = 20,
-        order: Order | None = Order.desc,
-    ) -> ListOpenAIResponseInputItem:
-        """List input items.
-
-        :param response_id: The ID of the response to retrieve input items for.
-        :param after: An item ID to list items after, used for pagination.
-        :param before: An item ID to list items before, used for pagination.
-        :param include: Additional fields to include in the response.
-        :param limit: A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
-        :param order: The order to return the input items in. Default is desc.
-        :returns: An ListOpenAIResponseInputItem.
-        """
-        ...
-
-    @webmethod(route="/responses/{response_id}", method="DELETE", level=LLAMA_STACK_API_V1)
-    async def delete_openai_response(self, response_id: str) -> OpenAIDeleteResponseObject:
-        """Delete a response.
-
-        :param response_id: The ID of the OpenAI response to delete.
-        :returns: An OpenAIDeleteResponseObject
-        """
-        ...
diff --git a/src/llama_stack_api/agents/__init__.py b/src/llama_stack_api/agents/__init__.py
new file mode 100644
index 0000000000..e02be3f505
--- /dev/null
+++ b/src/llama_stack_api/agents/__init__.py
@@ -0,0 +1,38 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the terms described in the LICENSE file in
+# the root directory of this source tree.
+
+"""Agents API protocol and models.
+
+This module contains the Agents protocol definition for the OpenAI Responses API.
+Pydantic models are defined in llama_stack_api.agents.models.
+The FastAPI router is defined in llama_stack_api.agents.fastapi_routes.
+"""
+
+from . import fastapi_routes
+from .api import Agents
+from .models import (
+    CreateResponseRequest,
+    DeleteResponseRequest,
+    ListResponseInputItemsRequest,
+    ListResponsesRequest,
+    ResponseGuardrail,
+    ResponseGuardrailSpec,
+    ResponseItemInclude,
+    RetrieveResponseRequest,
+)
+
+__all__ = [
+    "Agents",
+    "CreateResponseRequest",
+    "DeleteResponseRequest",
+    "ListResponseInputItemsRequest",
+    "ListResponsesRequest",
+    "ResponseGuardrail",
+    "ResponseGuardrailSpec",
+    "ResponseItemInclude",
+    "RetrieveResponseRequest",
+    "fastapi_routes",
+]
diff --git a/src/llama_stack_api/agents/api.py b/src/llama_stack_api/agents/api.py
new file mode 100644
index 0000000000..4faaa29717
--- /dev/null
+++ b/src/llama_stack_api/agents/api.py
@@ -0,0 +1,52 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the terms described in the LICENSE file in
+# the root directory of this source tree.
+
+from collections.abc import AsyncIterator
+from typing import Protocol, runtime_checkable
+
+from llama_stack_api.openai_responses import (
+    ListOpenAIResponseInputItem,
+    ListOpenAIResponseObject,
+    OpenAIDeleteResponseObject,
+    OpenAIResponseObject,
+    OpenAIResponseObjectStream,
+)
+
+from .models import (
+    CreateResponseRequest,
+    DeleteResponseRequest,
+    ListResponseInputItemsRequest,
+    ListResponsesRequest,
+    RetrieveResponseRequest,
+)
+
+
+@runtime_checkable
+class Agents(Protocol):
+    async def get_openai_response(
+        self,
+        request: RetrieveResponseRequest,
+    ) -> OpenAIResponseObject: ...
+
+    async def create_openai_response(
+        self,
+        request: CreateResponseRequest,
+    ) -> OpenAIResponseObject | AsyncIterator[OpenAIResponseObjectStream]: ...
+
+    async def list_openai_responses(
+        self,
+        request: ListResponsesRequest,
+    ) -> ListOpenAIResponseObject: ...
+
+    async def list_openai_response_input_items(
+        self,
+        request: ListResponseInputItemsRequest,
+    ) -> ListOpenAIResponseInputItem: ...
+
+    async def delete_openai_response(
+        self,
+        request: DeleteResponseRequest,
+    ) -> OpenAIDeleteResponseObject: ...
diff --git a/src/llama_stack_api/agents/fastapi_routes.py b/src/llama_stack_api/agents/fastapi_routes.py
new file mode 100644
index 0000000000..fa533c4b3b
--- /dev/null
+++ b/src/llama_stack_api/agents/fastapi_routes.py
@@ -0,0 +1,229 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the terms described in the LICENSE file in
+# the root directory of this source tree.
+
+"""FastAPI router for the Agents API.
+
+This module defines the FastAPI router for the Agents API using standard
+FastAPI route decorators.
+"""
+
+import asyncio
+import json
+from collections.abc import AsyncIterator
+from typing import Annotated, Any
+
+from fastapi import APIRouter, Body, Depends, HTTPException, Path, Query
+from fastapi.responses import StreamingResponse
+from pydantic import BaseModel
+
+from llama_stack_api.common.responses import Order
+from llama_stack_api.openai_responses import (
+    ListOpenAIResponseInputItem,
+    ListOpenAIResponseObject,
+    OpenAIDeleteResponseObject,
+    OpenAIResponseObject,
+)
+from llama_stack_api.router_utils import (
+    create_path_dependency,
+    create_query_dependency,
+    standard_responses,
+)
+from llama_stack_api.version import LLAMA_STACK_API_V1
+
+from .api import Agents
+from .models import (
+    CreateResponseRequest,
+    DeleteResponseRequest,
+    ListResponseInputItemsRequest,
+    ListResponsesRequest,
+    ResponseItemInclude,
+    RetrieveResponseRequest,
+)
+
+
+def create_sse_event(data: Any) -> str:
+    """Create a Server-Sent Event string from data."""
+    if isinstance(data, BaseModel):
+        data = data.model_dump_json()
+    else:
+        data = json.dumps(data)
+    return f"data: {data}\n\n"
+
+
+async def sse_generator(event_gen):
+    """Convert an async generator to SSE format.
+
+    This function iterates over an async generator and formats each yielded
+    item as a Server-Sent Event.
+    """
+    try:
+        async for item in event_gen:
+            yield create_sse_event(item)
+    except asyncio.CancelledError:
+        if hasattr(event_gen, "aclose"):
+            await event_gen.aclose()
+        raise  # Re-raise to maintain proper cancellation semantics
+    except Exception as e:
+        yield create_sse_event({"error": {"message": str(e)}})
+
+
+# Automatically generate dependency functions from Pydantic models
+get_retrieve_response_request = create_path_dependency(RetrieveResponseRequest)
+get_delete_response_request = create_path_dependency(DeleteResponseRequest)
+get_list_responses_request = create_query_dependency(ListResponsesRequest)
+
+
+# Manual dependency for ListResponseInputItemsRequest since it mixes Path and Query parameters
+async def get_list_response_input_items_request(
+    response_id: Annotated[str, Path(description="The ID of the response to retrieve input items for.")],
+    after: Annotated[
+        str | None,
+        Query(description="An item ID to list items after, used for pagination."),
+    ] = None,
+    before: Annotated[
+        str | None,
+        Query(description="An item ID to list items before, used for pagination."),
+    ] = None,
+    include: Annotated[
+        list[ResponseItemInclude] | None,
+        Query(description="Additional fields to include in the response."),
+    ] = None,
+    limit: Annotated[
+        int | None,
+        Query(
+            description="A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20."
+        ),
+    ] = 20,
+    order: Annotated[Order | None, Query(description="The order to return the input items in.")] = Order.desc,
+) -> ListResponseInputItemsRequest:
+    return ListResponseInputItemsRequest(
+        response_id=response_id,
+        after=after,
+        before=before,
+        include=include,
+        limit=limit,
+        order=order,
+    )
+
+
+def _http_exception_from_value_error(exc: ValueError) -> HTTPException:
+    """Convert implementation `ValueError` into an OpenAI-compatible HTTP error.
+
+    The compatibility OpenAI client maps HTTP 400 -> `BadRequestError`.
+    The existing API surface (and integration tests) expect "not found" cases
+    to be represented as a 400, not a 404.
+    """
+
+    detail = str(exc) or "Invalid value"
+    return HTTPException(status_code=400, detail=detail)
+
+
+def create_router(impl: Agents) -> APIRouter:
+    """Create a FastAPI router for the Agents API.
+
+    Args:
+        impl: The Agents implementation instance
+
+    Returns:
+        APIRouter configured for the Agents API
+    """
+    router = APIRouter(
+        prefix=f"/{LLAMA_STACK_API_V1}",
+        tags=["Agents"],
+        responses=standard_responses,
+    )
+
+    @router.get(
+        "/responses/{response_id}",
+        response_model=OpenAIResponseObject,
+        summary="Get a model response.",
+        description="Get a model response.",
+    )
+    async def get_openai_response(
+        request: Annotated[RetrieveResponseRequest, Depends(get_retrieve_response_request)],
+    ) -> OpenAIResponseObject:
+        try:
+            return await impl.get_openai_response(request)
+        except ValueError as exc:
+            raise _http_exception_from_value_error(exc) from exc
+
+    @router.post(
+        "/responses",
+        summary="Create a model response.",
+        description="Create a model response.",
+        status_code=200,
+        response_model=None,
+        responses={
+            200: {
+                "description": "An OpenAIResponseObject or a stream of OpenAIResponseObjectStream.",
+                "content": {
+                    "application/json": {"schema": {"$ref": "#/components/schemas/OpenAIResponseObject"}},
+                    "text/event-stream": {"schema": {"$ref": "#/components/schemas/OpenAIResponseObjectStream"}},
+                },
+            }
+        },
+    )
+    async def create_openai_response(
+        request: Annotated[CreateResponseRequest, Body(...)],
+    ) -> OpenAIResponseObject | StreamingResponse:
+        try:
+            result = await impl.create_openai_response(request)
+        except ValueError as exc:
+            raise _http_exception_from_value_error(exc) from exc
+
+        # For streaming responses, wrap in StreamingResponse for HTTP requests.
+        # The implementation is typed to return an `AsyncIterator` for streaming.
+        if isinstance(result, AsyncIterator):
+            return StreamingResponse(
+                sse_generator(result),
+                media_type="text/event-stream",
+            )
+
+        return result
+
+    @router.get(
+        "/responses",
+        response_model=ListOpenAIResponseObject,
+        summary="List all responses.",
+        description="List all responses.",
+    )
+    async def list_openai_responses(
+        request: Annotated[ListResponsesRequest, Depends(get_list_responses_request)],
+    ) -> ListOpenAIResponseObject:
+        try:
+            return await impl.list_openai_responses(request)
+        except ValueError as exc:
+            raise _http_exception_from_value_error(exc) from exc
+
+    @router.get(
+        "/responses/{response_id}/input_items",
+        response_model=ListOpenAIResponseInputItem,
+        summary="List input items.",
+        description="List input items.",
+    )
+    async def list_openai_response_input_items(
+        request: Annotated[ListResponseInputItemsRequest, Depends(get_list_response_input_items_request)],
+    ) -> ListOpenAIResponseInputItem:
+        try:
+            return await impl.list_openai_response_input_items(request)
+        except ValueError as exc:
+            raise _http_exception_from_value_error(exc) from exc
+
+    @router.delete(
+        "/responses/{response_id}",
+        response_model=OpenAIDeleteResponseObject,
+        summary="Delete a response.",
+        description="Delete a response.",
+    )
+    async def delete_openai_response(
+        request: Annotated[DeleteResponseRequest, Depends(get_delete_response_request)],
+    ) -> OpenAIDeleteResponseObject:
+        try:
+            return await impl.delete_openai_response(request)
+        except ValueError as exc:
+            raise _http_exception_from_value_error(exc) from exc
+
+    return router
diff --git a/src/llama_stack_api/agents/models.py b/src/llama_stack_api/agents/models.py
new file mode 100644
index 0000000000..c16fb8c4f4
--- /dev/null
+++ b/src/llama_stack_api/agents/models.py
@@ -0,0 +1,170 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the terms described in the LICENSE file in
+# the root directory of this source tree.
+
+"""Pydantic models for Agents API requests and responses.
+
+This module defines the request and response models for the Agents API
+using Pydantic with Field descriptions for OpenAPI schema generation.
+"""
+
+from enum import StrEnum
+
+from pydantic import BaseModel, ConfigDict, Field
+
+from llama_stack_api.common.responses import Order
+from llama_stack_api.openai_responses import (
+    OpenAIResponseInput,
+    OpenAIResponseInputTool,
+    OpenAIResponseInputToolChoice,
+    OpenAIResponsePrompt,
+    OpenAIResponseText,
+)
+
+
+class ResponseItemInclude(StrEnum):
+    """Specify additional output data to include in the model response."""
+
+    web_search_call_action_sources = "web_search_call.action.sources"
+    code_interpreter_call_outputs = "code_interpreter_call.outputs"
+    computer_call_output_output_image_url = "computer_call_output.output.image_url"
+    file_search_call_results = "file_search_call.results"
+    message_input_image_image_url = "message.input_image.image_url"
+    message_output_text_logprobs = "message.output_text.logprobs"
+    reasoning_encrypted_content = "reasoning.encrypted_content"
+
+
+class ResponseGuardrailSpec(BaseModel):
+    """Specification for a guardrail to apply during response generation."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    type: str
+    # TODO: more fields to be added for guardrail configuration
+
+
+ResponseGuardrail = str | ResponseGuardrailSpec
+
+
+class CreateResponseRequest(BaseModel):
+    """Request model for creating a response."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    input: str | list[OpenAIResponseInput] = Field(..., description="Input message(s) to create the response.")
+    model: str = Field(..., description="The underlying LLM used for completions.")
+    prompt: OpenAIResponsePrompt | None = Field(
+        default=None, description="Prompt object with ID, version, and variables."
+    )
+    instructions: str | None = Field(default=None, description="Instructions to guide the model's behavior.")
+    parallel_tool_calls: bool | None = Field(
+        default=True,
+        description="Whether to enable parallel tool calls.",
+    )
+    previous_response_id: str | None = Field(
+        default=None,
+        description="Optional ID of a previous response to continue from.",
+    )
+    conversation: str | None = Field(
+        default=None,
+        description="Optional ID of a conversation to add the response to.",
+    )
+    store: bool | None = Field(
+        default=True,
+        description="Whether to store the response in the database.",
+    )
+    stream: bool | None = Field(
+        default=False,
+        description="Whether to stream the response.",
+    )
+    temperature: float | None = Field(
+        default=None,
+        ge=0.0,
+        le=2.0,
+        description="Sampling temperature.",
+    )
+    text: OpenAIResponseText | None = Field(
+        default=None,
+        description="Configuration for text response generation.",
+    )
+    tool_choice: OpenAIResponseInputToolChoice | None = Field(
+        default=None,
+        description="How the model should select which tool to call (if any).",
+    )
+    tools: list[OpenAIResponseInputTool] | None = Field(
+        default=None,
+        description="List of tools available to the model.",
+    )
+    include: list[ResponseItemInclude] | None = Field(
+        default=None,
+        description="Additional fields to include in the response.",
+    )
+    max_infer_iters: int | None = Field(
+        default=10,
+        ge=1,
+        description="Maximum number of inference iterations.",
+    )
+    guardrails: list[ResponseGuardrail] | None = Field(
+        default=None,
+        description="List of guardrails to apply during response generation.",
+    )
+    max_tool_calls: int | None = Field(
+        default=None,
+        description="Max number of total calls to built-in tools that can be processed in a response.",
+    )
+    metadata: dict[str, str] | None = Field(
+        default=None,
+        description="Dictionary of metadata key-value pairs to attach to the response.",
+    )
+
+
+class RetrieveResponseRequest(BaseModel):
+    """Request model for retrieving a response."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    response_id: str = Field(..., min_length=1, description="The ID of the OpenAI response to retrieve.")
+
+
+class ListResponsesRequest(BaseModel):
+    """Request model for listing responses."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    after: str | None = Field(default=None, description="The ID of the last response to return.")
+    limit: int | None = Field(default=50, ge=1, le=100, description="The number of responses to return.")
+    model: str | None = Field(default=None, description="The model to filter responses by.")
+    order: Order | None = Field(
+        default=Order.desc,
+        description="The order to sort responses by when sorted by created_at ('asc' or 'desc').",
+    )
+
+
+class ListResponseInputItemsRequest(BaseModel):
+    """Request model for listing input items of a response."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    response_id: str = Field(..., min_length=1, description="The ID of the response to retrieve input items for.")
+    after: str | None = Field(default=None, description="An item ID to list items after, used for pagination.")
+    before: str | None = Field(default=None, description="An item ID to list items before, used for pagination.")
+    include: list[ResponseItemInclude] | None = Field(
+        default=None, description="Additional fields to include in the response."
+    )
+    limit: int | None = Field(
+        default=20,
+        ge=1,
+        le=100,
+        description="A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.",
+    )
+    order: Order | None = Field(default=Order.desc, description="The order to return the input items in.")
+
+
+class DeleteResponseRequest(BaseModel):
+    """Request model for deleting a response."""
+
+    model_config = ConfigDict(extra="forbid")
+
+    response_id: str = Field(..., min_length=1, description="The ID of the OpenAI response to delete.")
diff --git a/tests/integration/conftest.py b/tests/integration/conftest.py
index 9a88e8d384..131d4c00b0 100644
--- a/tests/integration/conftest.py
+++ b/tests/integration/conftest.py
@@ -347,9 +347,6 @@ def pytest_generate_tests(metafunc):
     metafunc.parametrize(params, value_combinations, scope="session", ids=test_ids if test_ids else None)
 
 
-pytest_plugins = ["tests.integration.fixtures.common"]
-
-
 def pytest_ignore_collect(path: str, config: pytest.Config) -> bool:
     """Skip collecting paths outside the selected suite roots for speed."""
     suite = config.getoption("--suite")
diff --git a/tests/unit/conftest.py b/tests/unit/conftest.py
index 893cc4a7d2..50aa3fcfcc 100644
--- a/tests/unit/conftest.py
+++ b/tests/unit/conftest.py
@@ -24,6 +24,3 @@ def pytest_sessionstart(session) -> None:
 def suppress_httpx_logs(caplog):
     """Suppress httpx INFO logs for all unit tests"""
     caplog.set_level(logging.WARNING, logger="httpx")
-
-
-pytest_plugins = ["tests.unit.fixtures"]
diff --git a/tests/unit/core/routers/test_agents_router.py b/tests/unit/core/routers/test_agents_router.py
new file mode 100644
index 0000000000..6a5a054403
--- /dev/null
+++ b/tests/unit/core/routers/test_agents_router.py
@@ -0,0 +1,343 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+#
+# This source code is licensed under the terms described in the LICENSE file in
+# the root directory of this source tree.
+
+from unittest.mock import AsyncMock
+
+import pytest
+from fastapi import FastAPI, HTTPException
+
+from llama_stack.core.server.fastapi_router_registry import build_fastapi_router
+from llama_stack_api import Agents, Api
+from llama_stack_api.agents.models import (
+    CreateResponseRequest,
+    DeleteResponseRequest,
+    ListResponseInputItemsRequest,
+    ListResponsesRequest,
+    RetrieveResponseRequest,
+)
+from llama_stack_api.openai_responses import (
+    ListOpenAIResponseInputItem,
+    ListOpenAIResponseObject,
+    OpenAIDeleteResponseObject,
+    OpenAIResponseObject,
+)
+
+
+def test_openapi_create_response_advertises_json_and_sse_200():
+    """Regression test for OpenAPI shape of POST /v1/responses.
+
+    We expect:
+    - 200 response (not 204)
+    - both JSON and SSE content types documented
+    """
+
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    schema = app.openapi()
+
+    post = schema["paths"]["/v1/responses"]["post"]
+    responses = post["responses"]
+    assert "200" in responses
+    assert "204" not in responses
+
+    content = responses["200"]["content"]
+    assert "application/json" in content
+    assert "text/event-stream" in content
+
+    assert content["application/json"]["schema"]["$ref"] == "#/components/schemas/OpenAIResponseObject"
+    assert content["text/event-stream"]["schema"]["$ref"] == "#/components/schemas/OpenAIResponseObjectStream"
+
+
+async def test_create_response_returns_sse_streaming_response_when_impl_streams():
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+
+    async def _stream():
+        yield {"type": "response.output_text.delta", "delta": "hello"}
+
+    impl.create_openai_response.return_value = _stream()
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    create = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses" and "POST" in getattr(r, "methods", set())
+    )
+
+    request = CreateResponseRequest(input="hi", model="test", stream=True)
+    response = await create(request)
+
+    assert response.media_type == "text/event-stream"
+
+
+async def test_create_response_maps_value_error_to_400_http_exception():
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+    impl.create_openai_response.side_effect = ValueError("not found")
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    create = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses" and "POST" in getattr(r, "methods", set())
+    )
+
+    request = CreateResponseRequest(input="hi", model="test", stream=False)
+
+    with pytest.raises(HTTPException) as excinfo:
+        await create(request)
+
+    assert excinfo.value.status_code == 400
+    assert excinfo.value.detail == "not found"
+
+
+async def test_create_response_returns_json_for_non_streaming():
+    """Test POST /v1/responses returns OpenAIResponseObject when stream=False."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+
+    expected_response = OpenAIResponseObject(
+        id="resp_123",
+        created_at=1234567890,
+        model="test-model",
+        object="response",
+        output=[],
+        status="completed",
+    )
+    impl.create_openai_response.return_value = expected_response
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    create = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses" and "POST" in getattr(r, "methods", set())
+    )
+
+    request = CreateResponseRequest(input="hi", model="test", stream=False)
+    response = await create(request)
+
+    assert not hasattr(response, "media_type")  # Not a StreamingResponse
+    assert response.id == "resp_123"
+    assert response.status == "completed"
+
+
+async def test_sse_format_is_correct():
+    """Test that streaming responses produce valid SSE format."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+
+    async def _stream():
+        yield {"type": "response.output_text.delta", "delta": "hello"}
+        yield {"type": "response.completed"}
+
+    impl.create_openai_response.return_value = _stream()
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    create = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses" and "POST" in getattr(r, "methods", set())
+    )
+
+    request = CreateResponseRequest(input="hi", model="test", stream=True)
+    response = await create(request)
+
+    # Collect SSE events from the body iterator
+    events = []
+    async for chunk in response.body_iterator:
+        events.append(chunk)
+
+    assert len(events) == 2
+    # Verify SSE format: "data: {...}\n\n"
+    assert events[0].startswith("data: ")
+    assert events[0].endswith("\n\n")
+    assert '"type": "response.output_text.delta"' in events[0]
+
+
+async def test_get_response_returns_response_object():
+    """Test GET /v1/responses/{response_id} returns OpenAIResponseObject."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+
+    expected_response = OpenAIResponseObject(
+        id="resp_123",
+        created_at=1234567890,
+        model="test-model",
+        object="response",
+        output=[],
+        status="completed",
+    )
+    impl.get_openai_response.return_value = expected_response
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    get_endpoint = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses/{response_id}" and "GET" in getattr(r, "methods", set())
+    )
+
+    request = RetrieveResponseRequest(response_id="resp_123")
+    response = await get_endpoint(request)
+
+    assert response.id == "resp_123"
+    assert response.status == "completed"
+    impl.get_openai_response.assert_awaited_once()
+
+
+async def test_get_response_maps_value_error_to_400():
+    """Test GET /v1/responses/{response_id} maps ValueError to HTTP 400."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+    impl.get_openai_response.side_effect = ValueError("Response not found")
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    get_endpoint = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses/{response_id}" and "GET" in getattr(r, "methods", set())
+    )
+
+    request = RetrieveResponseRequest(response_id="nonexistent")
+
+    with pytest.raises(HTTPException) as excinfo:
+        await get_endpoint(request)
+
+    assert excinfo.value.status_code == 400
+    assert "not found" in excinfo.value.detail.lower()
+
+
+async def test_list_responses_returns_list():
+    """Test GET /v1/responses returns ListOpenAIResponseObject."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+
+    expected_response = ListOpenAIResponseObject(
+        object="list",
+        data=[],
+        has_more=False,
+        first_id="resp_first",
+        last_id="resp_last",
+    )
+    impl.list_openai_responses.return_value = expected_response
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    list_endpoint = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses" and "GET" in getattr(r, "methods", set())
+    )
+
+    request = ListResponsesRequest()
+    response = await list_endpoint(request)
+
+    assert response.object == "list"
+    assert response.has_more is False
+    impl.list_openai_responses.assert_awaited_once()
+
+
+async def test_list_input_items_returns_items():
+    """Test GET /v1/responses/{response_id}/input_items returns input items."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+
+    expected_response = ListOpenAIResponseInputItem(
+        object="list",
+        data=[],
+    )
+    impl.list_openai_response_input_items.return_value = expected_response
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    list_endpoint = next(
+        r.endpoint for r in router.routes if getattr(r, "path", None) == "/v1/responses/{response_id}/input_items"
+    )
+
+    request = ListResponseInputItemsRequest(response_id="resp_123")
+    response = await list_endpoint(request)
+
+    assert response.object == "list"
+    impl.list_openai_response_input_items.assert_awaited_once()
+
+
+async def test_delete_response_returns_confirmation():
+    """Test DELETE /v1/responses/{response_id} returns deletion confirmation."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+
+    expected_response = OpenAIDeleteResponseObject(
+        id="resp_123",
+        object="response",
+        deleted=True,
+    )
+    impl.delete_openai_response.return_value = expected_response
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    delete_endpoint = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses/{response_id}" and "DELETE" in getattr(r, "methods", set())
+    )
+
+    request = DeleteResponseRequest(response_id="resp_123")
+    response = await delete_endpoint(request)
+
+    assert response.id == "resp_123"
+    assert response.deleted is True
+    impl.delete_openai_response.assert_awaited_once()
+
+
+async def test_delete_response_maps_value_error_to_400():
+    """Test DELETE /v1/responses/{response_id} maps ValueError to HTTP 400."""
+    app = FastAPI()
+    impl = AsyncMock(spec=Agents)
+    impl.delete_openai_response.side_effect = ValueError("Response not found")
+
+    router = build_fastapi_router(Api.agents, impl)
+    assert router is not None
+    app.include_router(router)
+
+    delete_endpoint = next(
+        r.endpoint
+        for r in router.routes
+        if getattr(r, "path", None) == "/v1/responses/{response_id}" and "DELETE" in getattr(r, "methods", set())
+    )
+
+    request = DeleteResponseRequest(response_id="nonexistent")
+
+    with pytest.raises(HTTPException) as excinfo:
+        await delete_endpoint(request)
+
+    assert excinfo.value.status_code == 400
diff --git a/tests/unit/providers/agents/meta_reference/test_openai_responses.py b/tests/unit/providers/agents/meta_reference/test_openai_responses.py
index 857043bfec..357ba4c26a 100644
--- a/tests/unit/providers/agents/meta_reference/test_openai_responses.py
+++ b/tests/unit/providers/agents/meta_reference/test_openai_responses.py
@@ -30,9 +30,9 @@
     OpenAIFile,
     OpenAIFileObject,
     OpenAISystemMessageParam,
+    Order,
     Prompt,
 )
-from llama_stack_api.agents import Order
 from llama_stack_api.inference import (
     OpenAIAssistantMessageParam,
     OpenAIChatCompletionContentPartTextParam,
diff --git a/tests/unit/providers/agents/meta_reference/test_openai_responses_conversations.py b/tests/unit/providers/agents/meta_reference/test_openai_responses_conversations.py
index ac4681a45c..f497252246 100644
--- a/tests/unit/providers/agents/meta_reference/test_openai_responses_conversations.py
+++ b/tests/unit/providers/agents/meta_reference/test_openai_responses_conversations.py
@@ -7,6 +7,10 @@
 
 import pytest
 
+# Fixtures imported from test_openai_responses via root conftest.py for pytest 8.4+ compatibility
+from llama_stack.providers.inline.agents.meta_reference.responses.openai_responses import (
+    OpenAIResponsesImpl,
+)
 from llama_stack_api.common.errors import (
     ConversationNotFoundError,
     InvalidConversationIdError,
@@ -22,13 +26,6 @@
     OpenAIResponseOutputMessageContentOutputText,
 )
 
-# Import existing fixtures from the main responses test file
-pytest_plugins = ["tests.unit.providers.agents.meta_reference.test_openai_responses"]
-
-from llama_stack.providers.inline.agents.meta_reference.responses.openai_responses import (
-    OpenAIResponsesImpl,
-)
-
 
 @pytest.fixture
 def responses_impl_with_conversations(
